{
  
    
        "post0": {
            "title": "The Causality-Driven Company",
            "content": "The data-driven company . Much has been written on data being the new gold and enterprises needing to embrace big data and machine learning, enabling enterprises in three important ways: . Utilizing existing resources more efficiently thus reducing cost, | Increasing success rates thus increasing revenue, and | Developing entirely new business models thus diversifying their business. | Enterprises are said to be data-driven once their entire organization is data-first, meaning: . Both their core processes (e.g. production lines) and supporting processes (e.g. recruiting) are quantifiable, | Business and unit performance is quantifiable with transparent metrics, | All business units have a data mindset, i.e. are driven by testable hypothesis and have the technological means to do so, and | The entire organization, top to bottom, embraces measurability and hypothesis-driven progress. | Machine learning applications for the data-driven company . There a numerous applications of big data and machine learning in industry and the enterprise. Common applications include: . Predictive maintenance, | Demand forecasting, | Optimal pricing, | Intelligent routing, | Data-driven customer support (e.g. chatbots), amd | Churn prediction. | . Another key application in industry is smart manufacturing which promises to decrease cost and increase both production line output and quality. . Smart manufacturing . Product quality prediction . . Consider our production line, pictured above, which consists of a number of workstations that the different parts of our product need to pass through before being assembled into our final product. . The various workstations can be described by parameters, e.g. how much pressure or force is applied, how fast a tool is spinning, or categories such as the type of drill used. . Our fictional production line above has four production parameters spread across its workstations: A, B, C, and D. . At the far end of our production the quality of our assembled product is quantified in the quality assurance gateway (QA). Here, either an employee or a machine visually inspects or physically tests our product to gauge its quality. . Since our company is data-driven we collect data on every item coming off our production line. Such a data set may look as follows: . Product ID Timestamp A B C D QA . … | … | … | … | … | … | … | . 123 | 2020-05-10 14:35:00 | 1.2 | 102.1 | ZZ | YY | 0.98 | . 456 | 2020-05-10 16:20:00 | 1.1 | 104.3 | ZZ | XX | 0.96 | . 789 | 2020-05-11 06:04:00 | 1.3 | 100.6 | ZX | YY | 0.83 | . 012 | 2020-05-11 10:45:00 | 1.1 | 101.3 | ZX | YZ | 0.92 | . … | … | … | … | … | … | … | . Parameters A and B of our production line are numerical features while parameters C and D are categorical features. Our quality measurement QA is a numerical target variable which takes values between 0 and 1 with the latter representing the highest possible product quality. . Provided that we collect enough production line data, machine learning offers a rich toolbox for predicting either the QA score or whether it our QA score will lie above a certain threshold. The former case is called regression while the latter is called classification. . Machine learning models at our disposal include: . Linear and polynomial regression, | (Deep) neural networks / deep learning, | Classification and regression trees (CART), | Ensemble methods such as random forests and gradient tree boosting, | Logistic regression, | Support vector machines, | Gaussian processes, and | Naive Bayes. | . These machine learning models allow us to learn the, potentially, highly complex relationship between production line parameters A, B, C, and D and product quality, QA. . So training a machine learning on our production line data we get something like this: . QA = model(A, B, C, D) . Where our model predicts the quality of our final product based on the parameters of our production line. . Using our prediction model . Imagine we manage to train a highly accurate machine learning model on our production line that - what do we do with it? . Sure, if measuring the quality of our product (QA) is prohibitively expensive we could cut cost and just use our model to determine the quality score for items coming off our production line. . We could also try and adapt our model so that it provides accurate quality score predictions early in the production cycle for a given item - thus allowing us to cancel substandard items before they consume resources unnecessarily. . But really, what we are most likely interested in is: How should I tweak individual production parameters to increase product quality? . Issue with our prediction model . Unfortunately, our machine learning model does not allow us to answer our core question. Our machine learning model does not know how changing one individual production parameter affects the quality of our product. . Even though we write our model as . QA = model(A, B, C, D) . what this really means is . QA = model(complete set of production line parameters). . So imagine the parameters of our production line are set to the following values: . A = a, B = b, C = c, and D = d or parameter set (a, b, c, d). . Now if we wanted to predicted the quality score of our product with parameter A increased by 10%, i.e. . (1.1 x a, b, c, d) . then we would need to hope that we already produced an item with parameters very similar to all four target parameter values, i.e. (1.1 x a, b, c, d), otherwise our model would provide us with, possibly, widely inaccurate predictions of our quality score. . In essence: Our machine learning model learns the relationship between the entire set of production parameters and quality score - hence our model does not learn the individual impact of each production parameter on the quality score. . This issue pertains to all machine learning models: They learn the bulk - not individual - contribution of all observable input variables (or features) to the target value. . The missing piece: causality . What is causality? . To disentangle their individual contributions, we first need to account for how our production parameters influence one another and the quality score. . To understand the connections between the parameters of our production line and the quality score we pop down to the production floor and chat with our colleagues manning the individual workstations and their foremen. From them we learn that: . When the stiffness of the raw material (A) increases then downstream workers increase the pressure applied on the item (B), | Depending on the stiffness of the raw material (A) the type of downstream drill (C) is chosen, | The worker choosing the type of drill (C) also informs the worker pressing our item (B) since they need to balance bore friction and metal stiffness, | The worker applying a durable finish (D) to our product is not affected by the choices of their co-workers. | . Armed with these insights from our production floor we draw up the following causal model - showing the connections among our production parameters and the quality of our product: . . The issue with causality: Machine learning models cannot help us predict impact of surgical changes . One of our main uses for machine learning models is predicting the effect of isolated changes to our processes or organization. . However, since the processes that generate the data we train machine learning models on are oftentimes structured our models cannot learn the impact of isolated changes to the data-generating process. . Take the causal model describing our production line as an example: . In reality, parameter A does not affect QA directly but only indirectly via B and C - thus in our recorded production data these three variables are always coupled i.e. change in tandem, | Parameter B is dependent on C so our observed data set likely does not include data points where parameter B is tweaked in isolation. | . Now, we could say “We’ll just go down to the production floor and run experiments where we alter individual parameters in isolation and record the output quality score”. . This approach, called randomized experiment or randomized controlled trial, is oftentimes undesirable or simply impossible: . In the case of our production line (and smart manufacturing in general), randomly assigning production parameters to measure their individual impact would be both very time consuming and prohibitively expensive - imagine all the low-quality items that would come off the production line, | In e-commerce, for smart pricing, it is in fact prohibited by law to price the same item differently for different customers in the same market thus making, | In customer care it would be highly undesirable to treat some customers potentially poorly just to see how their purchase behavior changes, | In predictive maintenance it would be questionable or prohibited not to perform best practice maintenance steps on individual items to see how their performance deteriorates differently, and | In online marketing it is ofentimes prohibitively complex, or impossible since the introduction of GDPR, to cleanly understand the effects of isolated marketing parameters on individual users. | Causal inference toolset . Causal inference offers a toolset that helps us disentangle causal structures and understand the impact of individual input variables: Do-Calculus. . Do-calculus encompasses a set of rules that allow us to ask questions such as: “How will the quality score QA of my product change if I increase production parameter A by 10%”. . QA = model(do(A = 1.1 x a), b, c, d) . Nomenclature of causality . Causal model, causal inference, causal discovery . Common pitfalls uncovered by causality considerations . Becoming a causality-driven company . .",
            "url": "https://georg.io/the_causality-driven_company",
            "relUrl": "/the_causality-driven_company",
            "date": " • May 10, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Decision intelligence from historical observations for optimal marketing resource use",
            "content": "Summary . Marketing is a key success and revenue driver in B2C markets: An appropriate message placed at the appropriate time with a prospective customer will increase your business success. . However, marketing is also a major cost driver for businesses: Marketing efforts that are too broad, target the wrong audience, or convey the wrong message waste resources. . In the case of direct marketing via phone conversations a key cost factor is the amount of time a sales call agent spends with the prospective customer on the phone. . In this article we explore, rudimentarily, direct marketing data of a Portuguese financial institution. . We explore the relationship between call duration and success (purchase of offered financial product), and show that consideration of customer-specific factors influences how you should allocate your marketing resources. . Our prototypical analysis can be usueful in devising data-driven marketing and sales strategies that offer decision intelligence for your call agents. . Fetch the data . For our prototype we use the openly accessible Bank Marketing Data Set from the UC Irvine Machine Learning Repository. . !wget --quiet https://archive.ics.uci.edu/ml/machine-learning-databases/00222/bank.zip . !unzip -oqq bank.zip . Load Python libraries . import graphviz import numpy as np import pandas as pd import seaborn as sns from sklearn.cluster import KMeans from sklearn.preprocessing import LabelEncoder . np.random.seed(42) . Prepare data . The data we work with here contain a number of categorical and numerical variables. To keep our analysis and prototype simple we will focus on only a handful and remove the remainder. . #collapse df = pd.read_csv(&#39;bank.csv&#39;, delimiter=&#39;;&#39;) df[&#39;success&#39;] = df[&#39;y&#39;] del df[&#39;y&#39;] df[&#39;success&#39;] = df[&#39;success&#39;].replace(&#39;no&#39;, 0) df[&#39;success&#39;] = df[&#39;success&#39;].replace(&#39;yes&#39;, 1) del df[&#39;education&#39;] del df[&#39;default&#39;] del df[&#39;housing&#39;] del df[&#39;loan&#39;] del df[&#39;contact&#39;] del df[&#39;day&#39;] del df[&#39;month&#39;] del df[&#39;campaign&#39;] del df[&#39;pdays&#39;] del df[&#39;previous&#39;] del df[&#39;poutcome&#39;] . . Our tabular data set now looks as follows: Each prospective (and in some cases eventual) customer whom a call agent conversed with fills a row. On each row we have numerical variables (age, account balance, duration of sales interaction) and categorical variables (job / employment status and marital status). Our data set contains 4,521 sales interactions. . df . age job marital balance duration success . 0 30 | unemployed | married | 1787 | 79 | 0 | . 1 33 | services | married | 4789 | 220 | 0 | . 2 35 | management | single | 1350 | 185 | 0 | . 3 30 | management | married | 1476 | 199 | 0 | . 4 59 | blue-collar | married | 0 | 226 | 0 | . ... ... | ... | ... | ... | ... | ... | . 4516 33 | services | married | -333 | 329 | 0 | . 4517 57 | self-employed | married | -3313 | 153 | 0 | . 4518 57 | technician | married | 295 | 151 | 0 | . 4519 28 | blue-collar | married | 1137 | 129 | 0 | . 4520 44 | entrepreneur | single | 1136 | 345 | 0 | . 4521 rows × 6 columns . High-level model: more is better . A blanket approach to marketing and sales may be: More resources lead to greater success. . So in the case of direct marketing on the phone we could expect that the more time we spend with a prospective customer on the phone, the bigger our success rate. . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G duration duration success success duration&#45;&gt;success To test our model, we discretize the duration of our interaction with the customer into six duration buckets: bucket 1 holds the shortest interactions while bucket 6 holds the longest interactions. . no_buckets = 6 df[&#39;duration_bucket&#39;] = pd.qcut(df[&#39;duration&#39;], no_buckets, labels=[f&#39;bucket {b + 1}&#39; for b in range(no_buckets)]) . df.groupby(&#39;duration_bucket&#39;).agg({&#39;success&#39;: &#39;mean&#39;}) . success . duration_bucket . bucket 1 0.003932 | . bucket 2 0.031662 | . bucket 3 0.048193 | . bucket 4 0.095174 | . bucket 5 0.155378 | . bucket 6 0.358090 | . Looking at the average success rate in each duration bucket shows us that there is positive correlation between the duration of a sales interaction and our success rate - just as our model predicted. . Hence, more marketing spend appears to lead to greater success in general. . From a data perspective this is a pretty disappointing result as we expect to glean more intelligent insights from all the data we collected. . Nuanced model: more isn&#39;t always better and there are always tradeoffs . Let&#39;s dig deeper into what is going on here: Yes, the duration of the interaction between call agent and prospective customer likely influences our success rate. . However, call agents also probably choose to spend more time on the phone with customers whose account balance is higher - hoping for a greater chance of a sale. That same account balance also likely influences how affine the customer is for spending more money on financial products. . Both present job status and marital status are also likely candidates for influencing an affinity for financial products. . And age of the customer probably influences both their job and marital status. . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G age age job job age&#45;&gt;job marital marital age&#45;&gt;marital balance balance job&#45;&gt;balance success success job&#45;&gt;success marital&#45;&gt;success duration duration balance&#45;&gt;duration balance&#45;&gt;success duration&#45;&gt;success Since a customer&#39;s account balance probably influences both how much time we spend with them on the phone and their likelihood of purchasing another financial product we will control for account balance. . We control for account balance by training a cluster algorithm that segments our data set into three groups of similar account balance. . df[&#39;job&#39;] = LabelEncoder().fit_transform(df[&#39;job&#39;]) df[&#39;marital&#39;] = LabelEncoder().fit_transform(df[&#39;marital&#39;]) . segmenter = KMeans(n_clusters=3, random_state=42) . df[&#39;segment&#39;] = segmenter.fit_predict(df[[&#39;balance&#39;]]) . df[&#39;segment&#39;] = df[&#39;segment&#39;].replace({0: &#39;low balance&#39;, 1: &#39;high balance&#39;, 2: &#39;medium balance&#39;}) . Looking at both the average account balance and age in our three segments, we notice that our clustering algorithm picked out low, medium, and high balance segments. . We also notice that average age correlates with average balance in these three segments hence our intuition codified in our above model seems valid. . df.groupby(&#39;segment&#39;).agg({&#39;age&#39;: &#39;mean&#39;, &#39;balance&#39;: &#39;mean&#39;}) . age balance . segment . high balance 44.542857 | 18361.771429 | . low balance 40.845577 | 543.930678 | . medium balance 42.911111 | 5202.864957 | . Now, what about the effectiveness of our marketing resources in each segment? . Visualizing our rate of success in the six duration buckets broken down by account balance segment we see a more nuanced picture: . Customers with low account balances really need to be worked on and only show success rates greater than 20% in the highest duration bucket 6, | customers with medium balances already show a greater than 20% purchase likelihood in duration bucket 4, and | customers with high balances actually max out in duration bucket 5 and drop below a 20% success rate in bucket 6. | . success_rates = df.groupby([&#39;segment&#39;, &#39;duration_bucket&#39;]).agg({&#39;success&#39;: &#39;mean&#39;}).reset_index() . sns.set(rc={&#39;figure.figsize&#39;: (10,6)}) sns.barplot( x=&#39;duration_bucket&#39;, y=&#39;success&#39;, hue=&#39;segment&#39;, data=success_rates, hue_order=[&#39;low balance&#39;, &#39;medium balance&#39;, &#39;high balance&#39;] ); . Our more nuanced model and analysis provide us with data-driven insights that provide actionable and testable advice: . We should probably re-evaluate whether low balance individuals are sensible targets for our marketing campaigns given how resource-intensive they are, | compute the profit and loss tradeoff between spending bucket 4 and bucket 6 resources on medium balance individuals, and | ensure that we do not overdo it with our calls for high balance individuals. | .",
            "url": "https://georg.io/2020/01/12/Optimal_resource_allocation_and_uplift_for_direct_marketing",
            "relUrl": "/2020/01/12/Optimal_resource_allocation_and_uplift_for_direct_marketing",
            "date": " • Jan 12, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Smart maintenance based on vehicle CAN bus data from scratch in Python",
            "content": "Summary . Equipment that behaves anomalously or breaks down unexpectedly is a major cost driver in manufacturing, logistics, public transport, and any other sector that relies on complex machinery. . A big promise of data analytics and machine learning in this space is to detect anomalies in machinery automatically and to alert their user of occurring faults. As an extension, the prediction of machinery faults and breakdowns is an important field of application. . Automated detection and prediction of machinery breakdown is a key algorithmic approach behind smart and predictive maintenance. . In this article we showcase a simple algorithmic approach for anomaly detection in the space of automated engine health detection. . Our approach here can be an interesting starting point for the development of smart telematics solutions for automated and predictive vehicle breakdown detection. . Fetch the data . We&#39;ll make use of an open data set of vehicle CAN bus data, called Automotive CAN bus data: An Example Dataset from the AEGIS Big Data Project. . A CAN bus is a local network of sensors and actuators in modern vehicles that provides a stream of data for all important signals of a vehicle - such as its present velocity, interior temperature, and potentially hundreds of other signals. . This data set encompasses time series data (traces) of various vehicles driven by different drivers. . Let&#39;s go ahead and download a data set for driver 1 and a data set for driver 2: . !wget --quiet https://zenodo.org/record/3267184/files/20181113_Driver1_Trip1.hdf . !wget --quiet https://zenodo.org/record/3267184/files/20181114_Driver2_Trip3.hdf . Load libraries . Here we import all necessary Python libraries for our analysis and algorithm: . import h5py from matplotlib import pyplot as plt import numpy as np import pandas as pd import seaborn as sns from sklearn.mixture import GaussianMixture . plt.rcParams[&#39;figure.figsize&#39;] = (10,10) sns.set(style=&quot;darkgrid&quot;) . Load vehicle data . Let&#39;s load the data for driver 1 and driver 2 into memory: . driver_1 = h5py.File(&#39;20181113_Driver1_Trip1.hdf&#39;, &#39;r&#39;) driver_2 = h5py.File(&#39;20181114_Driver2_Trip3.hdf&#39;, &#39;r&#39;) . Both files contain multiple subgroups of data, one of which is the aformentioned CAN bus: . list(driver_1.keys()) . [&#39;AI&#39;, &#39;CAN&#39;, &#39;GPS&#39;, &#39;Math&#39;, &#39;Plugins&#39;] . list(driver_2.keys()) . [&#39;AI&#39;, &#39;CAN&#39;, &#39;GPS&#39;, &#39;Math&#39;, &#39;Plugins&#39;] . Turn time series data into tables . The CAN bus data comes in serialized form - written out in series in a nested format. . To handle the CAN bus data more efficiently we&#39;ll turn it into tables that are easier to inspect and handle. . data_driver_1 = {} data_driver_2 = {} for channel_name, channel_data in driver_1[&#39;CAN&#39;].items(): data_driver_1[channel_name] = channel_data[:, 0] table_driver_1 = pd.DataFrame( data=data_driver_1, index=channel_data[:, 1] ) table_driver_1 = table_driver_1.loc[:, table_driver_1.nunique() &gt; 1] for channel_name, channel_data in driver_2[&#39;CAN&#39;].items(): data_driver_2[channel_name] = channel_data[:, 0] table_driver_2 = pd.DataFrame( data=data_driver_2, index=channel_data[:, 1] ) table_driver_2 = table_driver_2.loc[:, table_driver_2.nunique() &gt; 1] . The tabular data for driver 1 looks as follows - it holds 158,659 measured time points in 28 channels that we deem relevant: . table_driver_1 . AccPedal AirIntakeTemperature AmbientTemperature BoostPressure BrkVoltage ENG_Trq_DMD ENG_Trq_ZWR ENG_Trq_m_ex EngineSpeed_CAN EngineTemperature Engine_02_BZ Engine_02_CHK OilTemperature1 SCS_01_BZ SCS_01_CHK SCS_Cancel SCS_Tip_Down SCS_Tip_Set SCS_Tip_Up SteerAngle1 Trq_FrictionLoss Trq_Indicated VehicleSpeed WheelSpeed_FL WheelSpeed_FR WheelSpeed_RL WheelSpeed_RR Yawrate1 . 0.000000 0.0 | 31.5 | 8.0 | 0.97 | 1.0 | 18.0 | 20.0 | 27.000000 | 809.500000 | 93.0 | 6.000000 | 168.000000 | 82.0 | 9.000000 | 27.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 125.599998 | 29.0 | 27.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.190000 | . 0.050000 0.0 | 31.5 | 8.0 | 0.97 | 1.0 | 18.0 | 20.0 | 27.000000 | 809.500000 | 93.0 | 6.000000 | 168.000000 | 82.0 | 9.000000 | 27.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 125.599998 | 29.0 | 27.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.190000 | . 0.100000 0.0 | 31.5 | 8.0 | 0.97 | 1.0 | 18.0 | 20.0 | 27.000000 | 810.215759 | 93.0 | 10.331579 | 163.663162 | 82.0 | 9.333000 | 26.000999 | 0.0 | 0.0 | 0.0 | 0.0 | 125.599998 | 29.0 | 27.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.209900 | . 0.150000 0.0 | 31.5 | 8.0 | 0.97 | 1.0 | 18.0 | 20.0 | 27.384237 | 807.657654 | 93.0 | 9.605911 | 165.674881 | 82.0 | 9.833000 | 24.500999 | 0.0 | 0.0 | 0.0 | 0.0 | 125.599998 | 29.0 | 27.384237 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.204887 | . 0.200000 0.0 | 31.5 | 8.0 | 0.97 | 1.0 | 18.0 | 20.0 | 27.000000 | 805.500000 | 93.0 | 4.358586 | 172.641418 | 82.0 | 10.333000 | 24.333000 | 0.0 | 0.0 | 0.0 | 0.0 | 125.599998 | 28.0 | 27.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.200000 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 7932.700195 0.0 | 25.5 | 9.5 | 0.98 | 0.0 | 20.0 | 19.0 | 29.000000 | 797.500000 | 96.0 | 8.174129 | 84.825874 | 96.0 | 10.493239 | 24.493240 | 0.0 | 0.0 | 0.0 | 0.0 | 14.100000 | 29.0 | 29.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.130000 | . 7932.750000 0.0 | 25.5 | 9.5 | 0.98 | 0.0 | 19.0 | 19.0 | 29.000000 | 800.500000 | 96.0 | 13.164251 | 145.835754 | 96.0 | 10.993991 | 24.993992 | 0.0 | 0.0 | 0.0 | 0.0 | 14.100000 | 29.0 | 29.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.123609 | . 7932.799805 0.0 | 25.5 | 9.5 | 0.98 | 0.0 | 19.0 | 19.0 | 29.000000 | 797.790588 | 96.0 | 2.177665 | 156.822342 | 96.0 | 11.494000 | 27.469999 | 0.0 | 0.0 | 0.0 | 0.0 | 14.100000 | 28.0 | 29.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.178596 | . 7932.850098 0.0 | 25.5 | 9.5 | 0.98 | 0.0 | 19.0 | 19.0 | 29.000000 | 796.000000 | 96.0 | 7.180095 | 153.739334 | 96.0 | 11.994000 | 29.969999 | 0.0 | 0.0 | 0.0 | 0.0 | 14.100000 | 28.0 | 29.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.157268 | . 7932.899902 0.0 | 25.5 | 9.5 | 0.98 | 0.0 | 19.0 | 19.0 | 29.000000 | 794.500000 | 96.0 | 12.155440 | 146.844559 | 96.0 | 12.494247 | 30.494247 | 0.0 | 0.0 | 0.0 | 0.0 | 14.100000 | 28.0 | 29.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.140000 | . 158659 rows × 28 columns . The tabular data for driver 2 looks as follows - it holds 136,154 measured time points in 29 channels that we deem relevant: . table_driver_2 . AccPedal AirIntakeTemperature AmbientTemperature BoostPressure BrkVoltage ENG_Trq_DMD ENG_Trq_ZWR ENG_Trq_m_ex EngineSpeed_CAN EngineTemperature Engine_02_BZ Engine_02_CHK OilTemperature1 SCS_01_BZ SCS_01_CHK SCS_Cancel SCS_Tip_Down SCS_Tip_Restart SCS_Tip_Set SCS_Tip_Up SteerAngle1 Trq_FrictionLoss Trq_Indicated VehicleSpeed WheelSpeed_FL WheelSpeed_FR WheelSpeed_RL WheelSpeed_RR Yawrate1 . 0.000000 0.0 | 44.25 | 14.5 | 1.00 | 1.0 | 20.000000 | 21.0 | 30.000000 | 791.000000 | 96.0 | 6.000000 | 59.000000 | 94.0 | 10.000000 | 24.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.800000 | 31.0 | 30.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.140000 | . 0.050000 0.0 | 44.25 | 14.5 | 1.00 | 1.0 | 21.000000 | 20.0 | 30.000000 | 791.000000 | 96.0 | 10.688680 | 102.688683 | 94.0 | 10.000000 | 24.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.800000 | 31.0 | 30.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.120000 | . 0.100000 0.0 | 44.25 | 14.5 | 1.00 | 1.0 | 21.000000 | 20.0 | 30.000000 | 791.165039 | 96.0 | 4.381188 | 105.079208 | 94.0 | 10.110166 | 24.110165 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.800000 | 31.0 | 30.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.148120 | . 0.150000 0.0 | 44.25 | 14.5 | 1.00 | 1.0 | 21.000000 | 20.0 | 30.751268 | 787.500000 | 96.0 | 4.725888 | 104.725891 | 94.0 | 10.610916 | 24.610916 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.800000 | 31.0 | 30.751268 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.126384 | . 0.200000 0.0 | 44.25 | 14.5 | 1.00 | 1.0 | 21.000000 | 20.0 | 31.000000 | 791.000000 | 96.0 | 9.733668 | 101.733665 | 94.0 | 11.111500 | 25.557501 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 4.800000 | 31.0 | 31.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.135592 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 6807.450195 0.0 | 25.50 | 10.5 | 0.98 | 1.0 | 21.000000 | 21.0 | 31.000000 | 815.500000 | 94.5 | 10.693069 | 118.693069 | 95.0 | 5.992500 | 22.394501 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 37.400002 | 30.0 | 31.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.130326 | . 6807.500000 0.0 | 25.50 | 10.5 | 0.98 | 1.0 | 21.000000 | 21.0 | 30.682692 | 813.500000 | 94.5 | 5.192307 | 120.884613 | 95.0 | 0.100500 | 18.100500 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 37.400002 | 30.0 | 30.682692 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.150000 | . 6807.549805 0.0 | 25.50 | 10.5 | 0.98 | 1.0 | 21.000000 | 21.0 | 31.000000 | 816.000000 | 94.5 | 4.676768 | 120.676765 | 95.0 | 0.600500 | 18.600500 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 37.476120 | 30.0 | 31.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.140351 | . 6807.600098 0.0 | 25.50 | 10.5 | 0.98 | 1.0 | 20.323383 | 21.0 | 30.000000 | 819.587524 | 94.5 | 9.676617 | 74.726372 | 95.0 | 1.100550 | 18.698349 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 37.476616 | 30.0 | 30.000000 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.140000 | . 6807.649902 0.0 | 25.50 | 10.5 | 0.98 | 1.0 | 22.000000 | 21.0 | 30.722773 | 810.048096 | 94.5 | 14.693069 | 178.693069 | 95.0 | 1.600800 | 17.197599 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 37.500000 | 31.0 | 30.722773 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.129625 | . 136154 rows × 29 columns . Monitoring engine health . One use case of automated anomaly detection lies in checking the health status of a vehicle&#39;s engine. . Here we&#39;ll look at engine oil temperature as a function of velocity: As you&#39;ll notice in the below plots, oil temperature goes up with higher velocity and increased operating duration. . Engine 1 - healthy . Let&#39;s look at the engine of the vehicle of driver 1 where engine oil temperature appears normal as it keeps to within a certain band: . temperature_1 = table_driver_1[[&#39;VehicleSpeed&#39;, &#39;OilTemperature1&#39;]].copy() sns.relplot( x=&#39;index&#39;, y=&#39;value&#39;, hue=&#39;channel&#39;, kind=&#39;line&#39;, data=temperature_1.reset_index().melt(id_vars=&#39;index&#39;, var_name=&#39;channel&#39;) ); . Plotting engine oil temperature against vehicle velocity makes the correlation between the two metrics more apparent: . plot = sns.scatterplot( x=&#39;VehicleSpeed&#39;, y=&#39;OilTemperature1&#39;, data=temperature_1, color=&#39;blue&#39;, alpha=.1 ); plot.axes.set_ylim(0., 110.); . Engine 2 - unhealthy . Let&#39;s look at velocity and engine oil temperature for vehicle 2 (where I deliberately introduce an anomaly between 5000 and 5500 seconds of the trace). . You&#39;ll notice a spike in engine oil temperature which indicates unhealthy behavior of the vehicle&#39;s engine: . temperature_2 = table_driver_2[[&#39;VehicleSpeed&#39;, &#39;OilTemperature1&#39;]].copy() temperature_2.loc[5000:5500, &#39;OilTemperature1&#39;] *= 1.15 sns.relplot( x=&#39;index&#39;, y=&#39;value&#39;, hue=&#39;channel&#39;, kind=&#39;line&#39;, data=temperature_2.reset_index().melt(id_vars=&#39;index&#39;, var_name=&#39;channel&#39;) ); . plot = sns.scatterplot( x=&#39;VehicleSpeed&#39;, y=&#39;OilTemperature1&#39;, data=temperature_2, color=&#39;blue&#39;, alpha=.1 ); plot.axes.set_ylim(0., 140.); . Anomaly detection algorithm: learn what observations to expect to recognize anomalies . Let&#39;s look at the visual distribution of engine oil temperature and velocity again and directly compare vehicle 1 with vehicle 2. . temperature_1[&#39;vehicle&#39;] = &#39;vehicle 1&#39; temperature_2[&#39;vehicle&#39;] = &#39;vehicle 2&#39; combined = pd.concat([temperature_1, temperature_2], axis=0, sort=True) plot = sns.scatterplot( x=&#39;VehicleSpeed&#39;, y=&#39;OilTemperature1&#39;, data=combined, hue=&#39;vehicle&#39;, alpha=.1 ); plot.axes.set_ylim(0., 140.); . You&#39;ll notice that the engine oil temperature of vehicle 2 tends to be higher than of vehicle 1 and vehicle 2 shows an island of high engine oil temperature separated from the bulk of data points. . We can use this by modelling the distribution of value pairs velocity and oil temperature that we would usually expect to observe. . Let&#39;s model the expected distribution of data points with a model called Gaussian mixture models. This model fits a set of Gaussian distributions to the distribution of value pairs we observed for vehicle 1 - thus defining what a healthy distribution of values looks like. . model = GaussianMixture(n_components=4) . model.fit(temperature_1[[&#39;VehicleSpeed&#39;, &#39;OilTemperature1&#39;]]) . GaussianMixture(covariance_type=&#39;full&#39;, init_params=&#39;kmeans&#39;, max_iter=100, means_init=None, n_components=4, n_init=1, precisions_init=None, random_state=None, reg_covar=1e-06, tol=0.001, verbose=0, verbose_interval=10, warm_start=False, weights_init=None) . After training our model on the observations for vehicle 1, let&#39;s score the likelihood of observing each observation we have on vehicle 2: . temperature_2[&#39;health_score&#39;] = model.score_samples(temperature_2[[&#39;VehicleSpeed&#39;, &#39;OilTemperature1&#39;]]) . sns.relplot( x=&#39;index&#39;, y=&#39;value&#39;, hue=&#39;metric&#39;, kind=&#39;line&#39;, data=temperature_2[[&#39;OilTemperature1&#39;, &#39;health_score&#39;]].reset_index().melt(id_vars=&#39;index&#39;, var_name=&#39;metric&#39;) ); . To clarify what we did here: We learned what a healthy distribution of vehicle velocity and engine oil temperature looks like based on data from vehicle 1 and applied this model to data from vehicle 2. . Looking at the health score we compute for vehicle 2 we notice that our health score drops markdely between 5000 and 5500 seconds into our CAN bus trace - exactly where oil temperature spikes unhealthily. . While we would need to do a lot more validation and model calibration before we could use this approach in a live environment, we can already see the potential of this approach. . With this approach we can start devising data-driven products and services for smart telematics and smart maintenance applications. .",
            "url": "https://georg.io/2020/01/05/CAN_bus_anomaly_detection_gaussian_mixture_model",
            "relUrl": "/2020/01/05/CAN_bus_anomaly_detection_gaussian_mixture_model",
            "date": " • Jan 5, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Multiclass classification applied to a stream of documents in Python",
            "content": "Classifying Wikipedia Changes . I recently joined a Kaggle competition on multilabel text classification and have learned a ton from basic code that one of the competitors shared in the forums. . The code of the genereous competitor does logistic regression classification for multiple classes with stochastic gradient ascent. It is further well-suited for online learning as it uses the hashing trick to one-hot encode boolean, string, and categorial features. . To better understand these methods and tricks I here apply some of them to a multilabel problem I chose mostly for the easy access to a constant stream of training data: . All recent changes on Wikipedia are tracked on this special page where we can see a number of interesting features such as the length of the change, the contributor&#39;s username, the title of the changed article, and the contributor&#39;s comment for a given change. . Using the Wikipedia API to look at this stream of changes we can also see how contributors classify their changes as bot, minor, and new. Multiple label assignments are possible, so that one contribution may be classified as both bot and new. . Here I will listen to this stream of changes, extract four features (length of change, comment string, username, and article title), and train three logistic regression classifiers (one for each class) to predict the likelihood of a change belonging to each one of them. The training is done with the stochastic gradient ascent method. . One caveat: I am a complete novice when it comes to most of this stuff so please take everything that follows with a grain of salt - on the same note I would be forever grateful for any feedback especially of the critical kind so that I can learn and improve. . %matplotlib inline import matplotlib from matplotlib import pyplot as pt import requests import json from math import log, exp, sqrt from datetime import datetime, timedelta import itertools from collections import defaultdict . The API that Wikipedia offer to listen to the stream of recent changes is described here. . URL = (&#39;http://en.wikipedia.org/w/api.php?format=json&amp;action=query&amp;list=recentchanges&amp;rcprop=parsedcomment&#39; &#39;%7Ctimestamp%7Ctitle%7Cflags%7Cids%7Csizes%7Cflags%7Cuser&amp;rclimit=100&#39;) . The logistic regression classifier requires us to compute the dot product between a feature vector $ mathbf{x}$ and a weight vector $ mathbf{w}$. . $$ mathbf{w}^ text{T} mathbf{x} = w_0 x_0 + w_1 x_1 + w_2 x_2 + ldots + w_N x_N.$$ . As by convention, the bias of the model is encoded with feature $x_0 = 1$ for all observations - the only thing that will change about the $w_0 x_0$-term is weight $w_0$ upon training. The length of the article change is tracked with numerical feature $x_1$ which equals the number of character changes (hence $x_1$ is either positive or negative for text addition and removal respectively). . As in the Kaggle code that our code is mostly based upon, string features are one-hot encoded using the hashing trick: . The string features extracted for each observed article change are username, a parse of the comment, and the title of article. Since this is an online learning problem there is no way of knowing how many unique usernames, comment strings, and article titles are going to be observed. . With the hashing trick we decide ab initio that D_sparse-many unique values across these three features are sufficient to care about: Our one-hot encoded feature space has dimension D_sparse and can be represented as a D_sparse-dimensional vector filled with 0&#39;s and 1&#39;s (feature not present / present respectively). . The hash in hashing trick comes from the fact that we use a hash function to convert strings to integers. Suppose now that we chose D_sparse = 3 and our hash function produces hash(&quot;georg&quot;) = 0, hash(&quot;georgwalther&quot;) = 2, and hash(&quot;walther&quot;) = 3 for three observed usernames. . For username georg we get feature vector $[1, 0, 0]$ and for username georgwalther we get $[0, 0, 1]$. The hash function maps username walther outside our 3-dimensional feature space and to close this loop we not only use the hash function but also the modulus (which defines an equivalence relation?): . hash(&quot;georg&quot;) % D_sparse = 0, hash(&quot;georgwalther&quot;) % D_sparse = 2, and hash(&quot;walther&quot;) % D_sparse = 0 . This illustrates one downside of using the hashing trick since we will now map usernames georg and walther to the same feature vector $[1, 0, 0]$. We are therefore best adviced to choose a big D_sparse to avoid mapping different feature values to the same one-hot-encoded feature - but probably not too big to preserve memory. . For each article change observation we only map three string features into this D_sparse-dimensional one-hot-encoded feature space - out of D_sparse-many vector elements there will only ever be three ones among (D_sparse-3) zeros (if we do not map to the same vector index multiple times). We will therefore use sparse encoding for these feature vectors (hence the sparse in D_sparse). . We will also normalize the length of change on the fly using an online algorithm for mean and variance estimation. . D = 2 # number of non-sparse features D_sparse = 2**18 # number of sparsely-encoded features . def get_length_statistics(length, n, mean, M2): &quot;&quot;&quot; https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Incremental_algorithm &quot;&quot;&quot; n += 1 delta = length-mean mean += float(delta)/n M2 += delta*(length - mean) if n &lt; 2: return mean, 0., M2 variance = float(M2)/(n - 1) std = sqrt(variance) return mean, std, M2 def get_data(): X = [1., 0.] # bias term, length of edit X_sparse = [0, 0, 0] # hash of comment, hash of username, hash of title Y = [0, 0, 0] # bot, minor, new length_n = 0 length_mean = 0. length_M2 = 0. while True: r = requests.get(URL) r_json = json.loads(r.text)[&#39;query&#39;][&#39;recentchanges&#39;] for el in r_json: length = abs(el[&#39;newlen&#39;] - el[&#39;oldlen&#39;]) length_n += 1 length_mean, length_std, length_M2 = get_length_statistics(length, length_n, length_mean, length_M2) X[1] = (length - length_mean)/length_std if length_std &gt; 0. else length X_sparse[0] = abs(hash(&#39;comment_&#39; + el[&#39;parsedcomment&#39;])) % D_sparse X_sparse[1] = abs(hash(&#39;username_&#39; + el[&#39;user&#39;])) % D_sparse X_sparse[2] = abs(hash(&#39;title_&#39; + el[&#39;title&#39;])) % D_sparse Y[0] = 0 if el.get(&#39;bot&#39;) is None else 1 Y[1] = 0 if el.get(&#39;minor&#39;) is None else 1 Y[2] = 0 if el.get(&#39;new&#39;) is None else 1 yield Y, X, X_sparse . def predict(w, w_sparse, x, x_sparse): &quot;&quot;&quot; P(y = 1 | (x, x_sparse), (w, w_sparse)) &quot;&quot;&quot; wTx = 0. for i, val in enumerate(x): wTx += w[i] * val for i in x_sparse: wTx += w_sparse[i] # *1 if i in x_sparse try: wTx = min(max(wTx, -100.), 100.) res = 1./(1. + exp(-wTx)) except OverflowError: print wTx raise return res . def update(alpha, w, w_sparse, x, x_sparse, p, y): for i, val in enumerate(x): w[i] += (y - p) * alpha * val for i in x_sparse: w_sparse[i] += (y - p) * alpha # * feature[i] but feature[i] == 1 if i in x . K = 3 w = [[0.] * D for k in range(K)] w_sparse = [[0.] * D_sparse for k in range(K)] predictions = [0.] * K alpha = .1 . time0 = datetime.now() training_time = timedelta(minutes=10) ctr = 0 for y, x, x_sparse in get_data(): for k in range(K): p = predict(w[k], w_sparse[k], x, x_sparse) predictions[k] = float(p) update(alpha, w[k], w_sparse[k], x, x_sparse, p, y[k]) ctr += 1 # if ctr % 10000 == 0: # print &#39;samples seen&#39;, ctr # print &#39;sample&#39;, y # print &#39;predicted&#39;, predictions # print &#39;&#39; if (datetime.now() - time0) &gt; training_time: break . ctr . As we can see, we crunched through 106,401 article changes during our ten-minute online training. . It would be fairly hard to understand the link between the D_sparse-dimensional one-hot-encoded feature space and the observed / predicted classes. However we can still look at the influence that the length of the article change has on our classification problem . print w[0] print w[1] print w[2] . Here we can see that the weight of the length of change for class 0 (bot) is -1.12, for class 1 (minor) is -0.97, and for class 2 (new) is 2.11. . Intuitively this makes sense since many added characters (big positive change) should make classification as a minor change less likely and classification as a new article more likely: For an observed positive character count change $C$, $2.11 C$ will place us further to the right, and $-0.97 C$ further to the left along the $x$-axis of the sigmoid function: . (from http://gaelvaroquaux.github.io/scikit-learn-tutorial/supervised_learning.html#classification) . Further below we will see that the vast majority of bot changes are classified as minor changes hence we would expect to see correlation between the weights of this feature for these two classes. . To further evaluate our three classifiers, we observe another 10,000 Wikipedia changes and construct a confusion matrix. . no_test = 10000 test_ctr = 0 classes = {c: c_i for c_i, c in enumerate(list(itertools.product([0, 1], repeat=3)))} confusion_matrix = [[0 for j in range(len(classes))] for i in range(len(classes))] predicted = [0, 0, 0] for y, x, x_sparse in get_data(): for k in range(K): p = predict(w[k], w_sparse[k], x, x_sparse) predicted[k] = 1 if p &gt; .5 else 0 i = classes[tuple(y)] j = classes[tuple(predicted)] confusion_matrix[i][j] += 1 test_ctr +=1 if test_ctr &gt;= no_test: break . matplotlib.rcParams[&#39;font.size&#39;] = 15 fig = pt.figure(figsize=(11, 11)) pt.clf() ax = fig.add_subplot(111) ax.set_aspect(1) res = ax.imshow(confusion_matrix, cmap=pt.cm.jet, interpolation=&#39;nearest&#39;) cb = fig.colorbar(res) labels = [{v: k for k, v in classes.iteritems()}[i] for i in range(len(classes))] pt.xticks(range(len(classes)), labels) pt.yticks(range(len(classes)), labels) pt.show() . The confusion matrix shows actual classes along the vertical and predicted classes along the horizontal axis. . The vast majority of observed classes are $(0, 0, 0)$ and our classifiers get most of these right except that some are misclassified as $(0, 0, 1)$ and $(0, 1, 0)$. . All observed bot-related changes (classes starting with a $1$) are $(1, 1, 0)$ (i.e. minor bot-effected changes) and our classifiers get all of those right. .",
            "url": "https://georg.io/2014/10/10/Classifying_Wikipedia_Changes",
            "relUrl": "/2014/10/10/Classifying_Wikipedia_Changes",
            "date": " • Oct 10, 2014"
        }
        
    
  
    
        ,"post4": {
            "title": "Linkage discovery between scientific articles in Python and with graphs",
            "content": "PLOS Biology-Inspired PLOS Biology Articles . This past week I had my first encounter with the concept of graph databases which lend themselves perfectly to modeling and capturing linked data. . I started reading the free and brilliant book Graph Databases by Robinson, Webber, and Eifrem and began playing around with Python bulbs by James Thornton. . I further took the data set of 1754 PLOS Biology articles that I have examined on this blog multiple times and created a Rexster-based graph database from them. Apart from the obvious authors, DOIs, and titles I also extracted references to other PLOS Biology articles. . In this blog post I will examine these links between PLOS Biology articles. . Let us first take a look at my database to get an idea of what this looks like. . %matplotlib inline from matplotlib import pyplot . from bulbs.rexster import Graph, Config, REXSTER_URI . REXSTER_URI = &#39;http://localhost:8182/graphs/plos&#39; config = Config(REXSTER_URI) . g = Graph(config) . The label g now holds a reference to our graph database. . Python bulbs allows us to define classes for our data model which is something I did when creating this graph database in the first place. These are the node (vertex) types and edge (relationship) types I defined: . # Bulbs Models from bulbs.model import Node, Relationship from bulbs.property import String, Integer, DateTime, List class Author(Node): element_type = &#39;author&#39; name = String(nullable=False) class Article(Node): element_type = &#39;article&#39; title = String(nullable=False) published = DateTime() doi = String() class Authorship(Relationship): label = &#39;authored&#39; class Citation(Relationship): label = &#39;cites&#39; reference_count = Integer(nullable=False) tag = String() . This is a very basic model of PLOS Biology articles that captures nothing more than authorship (edges between authors and articles) and citations (edges between articles). . Some of these concepts can and should probably be decorated further: for instance Authorship edges could include author contributions (as provided at the bottom of most PLOS Biology articles). . g.add_proxy(&#39;authors&#39;, Author) g.add_proxy(&#39;articles&#39;, Article) g.add_proxy(&#39;authored&#39;, Authorship) g.add_proxy(&#39;cites&#39;, Citation) . Usually we would use Rexster/Bulbs-builtin functions that rely on some internal index but since that index seems to be broken for me right now I will simply collect all nodes and edges by hand and create Python dictionaries as indeces. . This is okay here to do since our database is very small but would likely be prohibitive for anything marginally bigger. . nodes = g.V edges = g.E . authors = {n.name: n for n in nodes if n.element_type == &#39;author&#39;} . authors.keys()[:10] . [u&#39;Shuguang Zhang&#39;, u&#39;Ernst Hafen&#39;, u&#39;Maren Brockmeyer&#39;, u&#39;Bruno Eschli&#39;, u&#39;David B. Gurevich&#39;, u&#39;Michael Lynch&#39;, u&#39;Alejandro Valbuena&#39;, u&#39;Claudia Rutte&#39;, u&#39;Matthew M Wyatt&#39;, u&#39;Brianna B. Williams&#39;] . articles = {n.doi: n for n in nodes if n.element_type == &#39;article&#39;} . articles.keys()[:10] . [u&#39;10.1371/journal.pbio.0040216&#39;, u&#39;10.1371/journal.pbio.0040215&#39;, u&#39;10.1371/journal.pbio.0040210&#39;, u&#39;10.1371/journal.pbio.0040368&#39;, u&#39;10.1371/journal.pbio.0040369&#39;, u&#39;10.1371/journal.pbio.0040362&#39;, u&#39;10.1371/journal.pbio.0040363&#39;, u&#39;10.1371/journal.pbio.0040360&#39;, u&#39;10.1371/journal.pbio.0020275&#39;, u&#39;10.1371/journal.pbio.0040366&#39;] . Let us now do a brief sanity check and count the number of PLOS Biology articles in our data set (this should equal 1754). . len(articles.keys()) . 1754 . Let us now pick an article at random and see how this article is connected to the remainder of the graph. . article = articles[&#39;10.1371/journal.pbio.1000584&#39;] . This is the title of article: . article.title . u&#39;Clusters of Temporal Discordances Reveal Distinct Embryonic Patterning Mechanisms in Drosophila and Anopheles&#39; . These are the edges pointing to this article: . list(article.inE()) . [&lt;Authorship: http://localhost:8182/graphs/plos/edges/21895&gt;, &lt;Authorship: http://localhost:8182/graphs/plos/edges/21893&gt;, &lt;Authorship: http://localhost:8182/graphs/plos/edges/21891&gt;] . There are three Authorship edges that point to this specific article. . To get the node at the base of a directed edge we can either query article.inE().inV() (i.e. the in-node of this edge) or simply ask for the in-node of the article node straight away - this should be equivalent! . for author in article.inV(): print author.name . Yury Goltsev Michael Levine Dmitri Papatsenko . A quick check online confirms that these are indeed the authors of article. . As I mentioned above, I also collected all references to other PLOS Biology articles in my data set and modeled those as Citation relationships (edges) between articles. . The article we are currently looking at has one such out-edge to another PLOS Biology article: . list(article.outE()) . [&lt;Citation: http://localhost:8182/graphs/plos/edges/29290&gt;] . for citation in article.outV(): print citation.title print [n.name for n in citation.inV() if n.element_type == &#39;author&#39;] print citation.doi . The Cell Cycle–Regulated Genes of Schizosaccharomyces pombe [u&#39;Saumyadipta Pyne&#39;, u&#39;Janet Leatherwood&#39;, u&#39;Anna Oliva&#39;, u&#39;Bruce Futcher&#39;, u&#39;Adam Rosebrock&#39;, u&#39;Steve Skiena&#39;, u&#39;Francisco Ferrezuelo&#39;, u&#39;Haiying Chen&#39;] 10.1371/journal.pbio.0030225 . As you can see above, querying our database for the authors of the PLOS Biology article that our current article (article) cites is simple. . How many PLOS Biology articles in our data set of 1754 articles cite other PLOS Biology articles? . (caveat: this only represents those citations that I detected when parsing my set of articles) . sum(1 for n in nodes if n.element_type == &#39;article&#39; and n.outV() &gt; 0) . 526 . I did not only extract citation edges between PLOS Biology articles but also counted how often such a citation occurs in the body of the article. . For our article and its one cited PLOS Biology article I counted: . for citation in article.outE(): print citation.reference_count . 2 . Just to verify this, look up article online (DOI = 10.1371/journal.pbio.0030225) and look for reference [4] which corresponds to this one cited PLOS Biology article. . Let us now take a look at the observed distribution of how often cited PLOS Biology articles are referenced in the main text of the citing PLOS Biology article. . citation_counts = [] for doi in articles.keys(): if articles[doi].outE(): for e in articles[doi].outE(): if e.label == &#39;cites&#39;: citation_counts.append(e.reference_count) . pyplot.hist(citation_counts, bins=range(20)) pyplot.xlabel(&#39;number of times cited&#39;) pyplot.ylabel(&#39;count&#39;) . &lt;matplotlib.text.Text at 0x9a293d0&gt; . This histogram has a surprisingly long tail. Let us take a look at some of the bigger values to see if these make sense. . def article_pp(article): authors = unicode(&#39;, &#39;.join([n.name for n in article.inV() if n.element_type == &#39;author&#39;])) s = (&#39;Title: %s n&#39; &#39;Authors: %s n&#39; &#39;DOI: %s&#39; % (article.title, authors, article.doi)) return s . for edge in edges: if edge.label == &#39;cites&#39;: if edge.reference_count &gt;= 21: print(&#39;Citer:&#39;) print(article_pp(edge.outV())) print(&#39;&#39;) print(&#39;Citee&#39;) print(article_pp(edge.inV())) print(&#39;&#39;) print(&#39;Citer cites citee %d times.&#39; % edge.reference_count) print(&#39;--&#39;) . Citer: Title: A Feedback Loop between Dynamin and Actin Recruitment during Clathrin-Mediated Endocytosis Authors: Marko Lampe, Christien J. Merrifield, Marcus J. Taylor DOI: 10.1371/journal.pbio.1001302 Citee Title: A High Precision Survey of the Molecular Dynamics of Mammalian Clathrin-Mediated Endocytosis Authors: Marcus J. Taylor, David Perrais, Christien J. Merrifield DOI: 10.1371/journal.pbio.1000604 Citer cites citee 21 times. -- Citer: Title: H2A.Z-Mediated Localization of Genes at the Nuclear Periphery Confers Epigenetic Memory of Previous Transcriptional State Authors: Yvonne Fondufe-Mittendorf, Sara Ahmed, Jason H Brickner, Donna Garvey Brickner, Jonathan Widom, Ivelisse Cajigas, Pei-Chih Lee DOI: 10.1371/journal.pbio.0050081 Citee Title: Gene Recruitment of the Activated INO1 Locus to the Nuclear Membrane Authors: Peter Walter, Jason H Brickner DOI: 10.1371/journal.pbio.0020342 Citer cites citee 21 times. -- . Checking these by hand we verify that our counts are correct. . I think it is sensible to postulate that the more often one article cites another one, the more heavily the work presented in the citer was influenced by the citee. . There is certainly some cut-off at which importance stops increasing - my point is simply that citing another article multiple times in your manuscript probably means that you are basing your work at least partially on the article you cite. . In the above list we can already see that one article titled A sex-ratio Meiotic Drive System in Drosophila simulans. II: An X-linked Distorter is a clear follow-up to the article titled A sex-ratio Meiotic Drive System in Drosophila simulans. I: An Autosomal Suppressor. . One question I am interested in is: How inspired are authors by their own work (generally very inspired I would presume), and how inspiring are articles to a completely different group of authors? . In my opinion, if one group of authors inspires a completely different group of authors to carry out scientific work (be it to follw up, refute, or whatever) then that defines knowledge transfer and a point at which scientific knowledge really becomes worth the time and resources it cost to produce this knowledge in the first place. . (I am certain this statement can be refined further but roughly speaking this is what I think) . Let us redo the above histogram but exclude all cited PLOS Biology articles that have one or more authors in common with the citing article. . (one more bracketed caveat: When constructing my database I assumed that every author name occurs exactly once and is therefore unique - this is a heuristic that breaks easily) . def are_different_authors(article_1, article_2): authors_1 = [] authors_2 = [] for n in article_1.inV(): if n.element_type == &#39;author&#39;: authors_1.append(n.name) for n in article_2.inV(): if n.element_type == &#39;author&#39;: authors_2.append(n.name) authors_1 = set(authors_1) authors_2 = set(authors_2) return len(authors_1.intersection(authors_2)) == 0 . citation_counts = [] for edge in edges: if edge.label == &#39;cites&#39;: if are_different_authors(edge.inV(), edge.outV()): citation_counts.append(edge.reference_count) . pyplot.hist(citation_counts, bins=range(20)) pyplot.xlabel(&#39;number of times cited&#39;) pyplot.ylabel(&#39;count&#39;) . &lt;matplotlib.text.Text at 0xaa25310&gt; . This histogram does not look very different from the one above. . Let us take a look at data points in the tail: . for edge in edges: if edge.label == &#39;cites&#39;: if edge.reference_count &gt;= 16 and are_different_authors(edge.inV(), edge.outV()): print(&#39;Citer:&#39;) print(article_pp(edge.outV())) print(&#39;&#39;) print(&#39;Citee&#39;) print(article_pp(edge.inV())) print(&#39;&#39;) print(&#39;Citer cites citee %d times.&#39; % edge.reference_count) print(&#39;--&#39;) . Citer: Title: Lack of Support for the Association between GAD2 Polymorphisms and Severe Human Obesity Authors: Frank Geller, John P Kane, Raphael Merriman, Christian Vaisse, Winfried Rief, Robert Dent, Johannes Hebebrand, Björn Waldenmaier, Franck Mauvais-Jarvis, Anke Hinney, Michael M Swarbrick, Clive R Pullinger, Mary Malloy, Len A Pennacchio, Anna Ustaszewska, Denise L Lind, Wen-Chi Hsueh, Ruth McPherson, Martha M Cavazos, André Scherag, Pui-Yan Kwok DOI: 10.1371/journal.pbio.0030315 Citee Title: GAD2 on Chromosome 10p12 Is a Candidate Gene for Human Obesity Authors: Lynn Bekris, Valérie Vasseur-Delannoy, Philippe Boutin, Karin Séron, Philippe Froguel, Mohamed Chikri, Christian Dina, Laetitia Corset, M. Aline Charles, Séverine Dubois, Francis Vasseur, Janice Cabellon, Ake Lernmark, Bernadette Neve, Karine Clement DOI: 10.1371/journal.pbio.0000068 Citer cites citee 17 times. -- Citer: Title: Structural Basis of Rap Phosphatase Inhibition by Phr Peptides Authors: Alberto Marina, Francisca Gallego del Sol DOI: 10.1371/journal.pbio.1001511 Citee Title: Structural Basis of Response Regulator Inhibition by a Bacterial Anti-Activator Protein Authors: Matthew B. Neiditch, Melinda D. Baker DOI: 10.1371/journal.pbio.1001226 Citer cites citee 16 times. -- . As we can see the two article pairs in the tail of this updated distribution are linked with lower reference counts than what we observed before filtering for author disjointedness. . Now, how inspiring are PLOS Biology authors for other (different) PLOS Biology authors? . To answer this question, I would like to propose a measure that I have called Inspiration Factor in my own head for some time now and one variant of the model I have had in mind is this: . Inspiration is an increasing function of the number of authors (unrelated to you) that you inspired to carry out scientific work. . Since I do not want to count citations that are mentioned only once in the main text of an article, I will impose a threshold of at least three references. . I should refine the way I parse articles to account for the context that citations are referenced in. . Anyways, let us take a look at those PLOS Biology articles that have inspired at least three other PLOS Biology articles. . inspirators = [] for article in articles.values(): in_nodes = [] if article.inE(): for edge in article.inE(): if edge.label == &#39;cites&#39;: if are_different_authors(edge.inV(), edge.outV()) and edge.reference_count &gt;= 3: in_nodes.append([edge.outV(), edge.reference_count]) if len(in_nodes) &gt;= 3: inspirators.append([article, in_nodes]) . len(inspirators) . 2 . for inspirator in inspirators: print(&#39;Inspirator&#39;) print article_pp(inspirator[0]) print(&#39;&#39;) for el in inspirator[1]: print(&#39;Inspired Article&#39;) print article_pp(el[0]) print(&#39;Cites inspirator %d times.&#39; % el[1]) print(&#39;&#39;) print(&#39;--&#39;) print(&#39;&#39;) . Inspirator Title: The Evolution of Combinatorial Gene Regulation in Fungi Authors: Alexander D Johnson, Aaron D Hernday, Hao Li, Brian B Tuch, David J Galgoczy DOI: 10.1371/journal.pbio.0060038 Inspired Article Title: Biofilm Matrix Regulation by Candida albicans Zap1 Authors: Oliver R. Homann, Clarissa J. Nobile, Jean-Sebastien Deneault, Aaron P. Mitchell, Andre Nantel, Aaron D. Hernday, David R. Andes, Jeniel E. Nett, Alexander D. Johnson DOI: 10.1371/journal.pbio.1000133 Cites inspirator 3 times. Inspired Article Title: Evolutionary Tinkering with Conserved Components of a Transcriptional Regulatory Network Authors: Jaideep Mallick, Adnane Sellam, Hugo Lavoie, Hervé Hogues, Malcolm Whiteway, André Nantel DOI: 10.1371/journal.pbio.1000329 Cites inspirator 6 times. Inspired Article Title: Evolution of Phosphoregulation: Comparison of Phosphorylation Patterns across Yeast Species Authors: Assen Roguev, Dorothea Fiedler, Jonathan C. Trinidad, Wendell A. Lim, Pedro Beltrao, Kevan M. Shokat, Alma L. Burlingame, Nevan J. Krogan DOI: 10.1371/journal.pbio.1000134 Cites inspirator 3 times. -- Inspirator Title: Transcription Factors Bind Thousands of Active and Inactive Regions in the Drosophila Blastoderm Authors: Lisa Simirenko, Michael B Eisen, Mark Stapleton, Richard Weiszmann, Cris L. Luengo Hendriks, Tom Gingeras, Amy Beaton, Hou Cheng Chu, Xiao-yong Li, Terence P Speed, Victor Sementchenko, Mark D Biggin, Richard Bourgon, Stewart MacArthur, William Inwood, Susan E Celniker, Nobuo Ogawa, Venky N Iyer, David W Knowles, Daniel A Pollard, David Nix, Aaron Hechmer DOI: 10.1371/journal.pbio.0060027 Inspired Article Title: Target Genes of the MADS Transcription Factor SEPALLATA3: Integration of Developmental and Hormonal Pathways in the Arabidopsis Flower Authors: Cezary Smaczniak, Kerstin Kaufmann, Pawel Krajewski, Ruy Jauregui, Chiara A Airoldi, Gerco C Angenent, Jose M Muiño DOI: 10.1371/journal.pbio.1000090 Cites inspirator 3 times. Inspired Article Title: Evolutionary Plasticity of Polycomb/Trithorax Response Elements in Drosophila Species Authors: Arne Hauenschild, Leonie Ringrose, Renato Paro, Christina Altmutter, Marc Rehmsmeier DOI: 10.1371/journal.pbio.0060261 Cites inspirator 6 times. Inspired Article Title: Quantitative Analysis of the Drosophila Segmentation Regulatory Network Using Pattern Generating Potentials Authors: Sudhir Kumar, Susan E. Celniker, Ann S. Hammonds, Saurabh Sinha, Majid Kazemian, Charles Blatti, Noriko Wakabayashi-Ito, Scot A. Wolfe, Adam Richards, Michael McCutchan, Michael H. Brodsky DOI: 10.1371/journal.pbio.1000456 Cites inspirator 7 times. -- . And that is it for now. . I will expand my dataset to include more articles and think about how to enrich the data I extract from these articles. . One question that I am very intrigued to tackle soon is: How long of a chain of scientific discovery do you trigger? . I imagine that an article that lies at the beginning of a long chain of articles that inspired one another would have some significance. .",
            "url": "https://georg.io/2014/03/22/PLOS_Biology-Inspired_PLOS_Biology",
            "relUrl": "/2014/03/22/PLOS_Biology-Inspired_PLOS_Biology",
            "date": " • Mar 22, 2014"
        }
        
    
  
    
        ,"post5": {
            "title": "Topic discovery in scientific articles with Python",
            "content": "PLOS Biology Topics . Ever wonder what topics are discussed in PLOS Biology articles? Here I will apply an implementation of Latent Dirichlet Allocation (LDA) on a set of 1,754 PLOS Biology articles to work out what a possible collection of underlying topics could be. . I first read about LDA in Building Machine Learning Systems with Python co-authored by Luis Coelho. . LDA seems to have been first described by Blei et al. and I will use the implementation provided by gensim which was written by Radim Řehůřek. . import gensim . import plospy import os . import nltk . import cPickle as pickle . With the following lines of code we open, parse, and tokenize all 1,754 PLOS Biology articles in our collection. . As this takes a bit of time and memory, I carried out all of these steps once and stored the resulting data structures to my hard disk for later reuse - see further below. . all_names = [name for name in os.listdir(&#39;../plos/plos_biology/plos_biology_data&#39;) if &#39;.dat&#39; in name] . article_bodies = [] for name_i, name in enumerate(all_names): docs = plospy.PlosXml(&#39;../plos/plos_biology/plos_biology_data/&#39;+name) for article in docs.docs: article_bodies.append(article[&#39;body&#39;]) . We have 1,754 PLOS Biology articles in our collection: . len(article_bodies) . punkt_param = nltk.tokenize.punkt.PunktParameters() punkt_param.abbrev_types = set([&#39;et al&#39;, &#39;i.e&#39;, &#39;e.g&#39;, &#39;ref&#39;, &#39;c.f&#39;, &#39;fig&#39;, &#39;Fig&#39;, &#39;Eq&#39;, &#39;eq&#39;, &#39;eqn&#39;, &#39;Eqn&#39;, &#39;dr&#39;, &#39;Dr&#39;]) sentence_splitter = nltk.tokenize.punkt.PunktSentenceTokenizer(punkt_param) . sentences = [] for body in article_bodies: sentences.append(sentence_splitter.tokenize(body)) . articles = [] for body in sentences: this_article = [] for sentence in body: this_article.append(nltk.tokenize.word_tokenize(sentence)) articles.append(this_article) . pickle.dump(articles, open(&#39;plos_biology_articles_tokenized.list&#39;, &#39;w&#39;)) . articles = pickle.load(open(&#39;plos_biology_articles_tokenized.list&#39;, &#39;r&#39;)) . is_stopword = lambda w: len(w) &lt; 4 or w in nltk.corpus.stopwords.words(&#39;english&#39;) . Save each article as one list of tokens and filter out stopwords: . articles_unfurled = [] for article in articles: this_article = [] for sentence in article: this_article += [token.lower().encode(&#39;utf-8&#39;) for token in sentence if not is_stopword(token)] articles_unfurled.append(this_article) . pickle.dump(articles_unfurled, open(&#39;plos_biology_articles_unfurled.list&#39;, &#39;w&#39;)) . articles_unfurled = pickle.load(open(&#39;plos_biology_articles_unfurled.list&#39;, &#39;r&#39;)) . Dictionary and Corpus Creation . Create a dictionary of all words (tokens) that appear in our collection of PLOS Biology articles and create a bag of words object for each article (doc2bow). . dictionary = gensim.corpora.Dictionary(articles_unfurled) . dictionary.save(&#39;plos_biology.dict&#39;) . dictionary = gensim.corpora.dictionary.Dictionary().load(&#39;plos_biology.dict&#39;) . I noticed that the word figure occurs rather frequently in these articles, so let us exclude this and any other words that appear in more than half of the articles in this data set (thanks to Radim for pointing this out to me). . dictionary.filter_extremes() . corpus = [dictionary.doc2bow(article) for article in articles_unfurled] . gensim.corpora.MmCorpus.serialize(&#39;plos_biology_corpus.mm&#39;, corpus) . corpus = gensim.corpora.MmCorpus(&#39;plos_biology_corpus.mm&#39;) . model = gensim.models.ldamodel.LdaModel(corpus, id2word=dictionary, update_every=1, chunksize=100, passes=2, num_topics=20) . model.save(&#39;plos_biology.lda_model&#39;) . model = gensim.models.ldamodel.LdaModel.load(&#39;plos_biology.lda_model&#39;) . And these are the twenty topics we find in 1,754 PLOS Biology articles: . for topic_i, topic in enumerate(model.print_topics(20)): print(&#39;topic # %d: %s n&#39; % (topic_i+1, topic)) . Topics with Lemmatized Tokens . As we can notice, some of the tokens in the above topics are just singular and plural forms of the same word. . Let us see what topics we find after lemmatizing all of our tokens. . from nltk.stem import WordNetLemmatizer wnl = WordNetLemmatizer() articles_lemmatized = [] for article in articles_unfurled: articles_lemmatized.append([wnl.lemmatize(token) for token in article]) . pickle.dump(articles_lemmatized, open(&#39;plos_biology_articles_lemmatized.list&#39;, &#39;w&#39;)) . dictionary_lemmatized = gensim.corpora.Dictionary(articles_lemmatized) . dictionary_lemmatized.save(&#39;plos_biology_lemmatized.dict&#39;) . dictionary_lemmatized.filter_extremes() . corpus_lemmatized = [dictionary_lemmatized.doc2bow(article) for article in articles_lemmatized] . gensim.corpora.MmCorpus.serialize(&#39;plos_biology_corpus_lemmatized.mm&#39;, corpus_lemmatized) . model_lemmatized = gensim.models.ldamodel.LdaModel(corpus_lemmatized, id2word=dictionary_lemmatized, update_every=1, chunksize=100, passes=2, num_topics=20) . for topic_i, topic in enumerate(model_lemmatized.print_topics(20)): print(&#39;topic # %d: %s n&#39; % (topic_i+1, topic)) .",
            "url": "https://georg.io/2014/02/16/PLOS_Biology_Topics",
            "relUrl": "/2014/02/16/PLOS_Biology_Topics",
            "date": " • Feb 16, 2014"
        }
        
    
  
    
        ,"post6": {
            "title": "The Crank-Nicolson method combined with Runge-Kutta implemented from scratch in Python",
            "content": "Combining Crank-Nicolson and Runge-Kutta to Solve a Reaction-Diffusion System . We have already derived the Crank-Nicolson method to integrate the following reaction-diffusion system numerically: . $$ frac{ partial u}{ partial t} = D frac{ partial^2 u}{ partial x^2} + f(u),$$ . $$ frac{ partial u}{ partial x} Bigg|_{x = 0, L} = 0.$$ . Please refer to the earlier blog post for details. . In our previous derivation, we constructed the following stencil that we would go on to rearrange into a system of linear equations that we needed to solve every time step: . $$ frac{U_j^{n+1} - U_j^n}{ Delta t} = frac{D}{2 Delta x^2} left( U_{j+1}^n - 2 U_j^n + U_{j-1}^n + U_{j+1}^{n+1} - 2 U_j^{n+1} + U_{j-1}^{n+1} right) + f(U_j^n),$$ . where $j$ and $n$ are space and time grid points respectively. . Rearranging the above set of equations, we effectively integrate the reaction part with the explicit Euler method like so: . $$U_j^{n+1} = U_j^n + Delta t f(U_j^n).$$ . For functions $f$ that change rapidly for small changes in their input (stiff equations), using the explicit Euler method may pose stability problems unless we choose a sufficiently small $ Delta t$. . Therefore, I have been wondering if it would be possible to use a more sophisticated and stable numerical scheme to integrate the reaction part in the context of our Crank-Nicolson scheme. . For instance, to integrate the reaction part with the classical Runge-Kutta method, we would write out the following set of equations instead of the aforementioned one: . $$ frac{U_j^{n+1} - U_j^n}{ Delta t} = frac{D}{2 Delta x^2} left( U_{j+1}^n - 2 U_j^n + U_{j-1}^n + U_{j+1}^{n+1} - 2 U_j^{n+1} + U_{j-1}^{n+1} right) + frac{1}{6} left(k_1 + 2 k_2 + 2 k_3 + k_4 right),$$ . where . $$k_1 = f(U_j^n),$$ . $$k_2 = f left( U_j^n + frac{ Delta t}{2} k_1 right),$$ . $$k_3 = f left( U_j^n + frac{ Delta t}{2} k_2 right),$$ . $$k_4 = f left( U_j^n + Delta t k_3 right).$$ . Whether or not doing this makes sense theoretically I am not certain. But going ahead and implementing this to the numerical example we discussed earlier seems to suggest that this does work. . In the following Python code that is mostly a copy of our previous code we compare the time behaviour and accuracy (measured by mass conservation as our reaction diffusion system preserves mass) of the explicit Euler and Runge-Kutta 4 reaction integration. . We realize that the differences between the obtained numerical results are negligible and we shall compare both approaches with a stiffer reaction term another time. . We shall also take a look at more sophisticated measures of numerical stability another time. . %matplotlib inline import numpy from matplotlib import pyplot . numpy.set_printoptions(precision=3) . L = 1. J = 200 dx = float(L)/float(J-1) x_grid = numpy.array([j*dx for j in range(J)]) . T = 500 N = 1000 dt = float(T)/float(N-1) t_grid = numpy.array([n*dt for n in range(N)]) . D_v = float(10.)/float(100.) D_u = 0.01 * D_v k0 = 0.067 f = lambda u, v: dt*(v*(k0 + float(u*u)/float(1. + u*u)) - u) g = lambda u, v: -f(u,v) sigma_u = float(D_u*dt)/float((2.*dx*dx)) sigma_v = float(D_v*dt)/float((2.*dx*dx)) total_protein = 2.26 . no_high = 10 U = numpy.array([0.1 for i in range(no_high,J)] + [2. for i in range(0,no_high)]) V = numpy.array([float(total_protein-dx*sum(U))/float(J*dx) for i in range(0,J)]) . Let us take a look at the inhomogeneous initial condition: . pyplot.ylim((0., 2.1)) pyplot.xlabel(&#39;x&#39;) pyplot.ylabel(&#39;concentration&#39;) pyplot.plot(x_grid, U) pyplot.plot(x_grid, V) pyplot.show() . These are the matrices of our system of linear equations whose derivation we described earlier. . A_u = numpy.diagflat([-sigma_u for i in range(J-1)], -1) + numpy.diagflat([1.+sigma_u]+[1.+2.*sigma_u for i in range(J-2)]+[1.+sigma_u]) + numpy.diagflat([-sigma_u for i in range(J-1)], 1) B_u = numpy.diagflat([sigma_u for i in range(J-1)], -1) + numpy.diagflat([1.-sigma_u]+[1.-2.*sigma_u for i in range(J-2)]+[1.-sigma_u]) + numpy.diagflat([sigma_u for i in range(J-1)], 1) A_v = numpy.diagflat([-sigma_v for i in range(J-1)], -1) + numpy.diagflat([1.+sigma_v]+[1.+2.*sigma_v for i in range(J-2)]+[1.+sigma_v]) + numpy.diagflat([-sigma_v for i in range(J-1)], 1) B_v = numpy.diagflat([sigma_v for i in range(J-1)], -1) + numpy.diagflat([1.-sigma_v]+[1.-2.*sigma_v for i in range(J-2)]+[1.-sigma_v]) + numpy.diagflat([sigma_v for i in range(J-1)], 1) . Function f_vec_ee returns the explicit Euler time step vector while f_vec_rk returns the vector obtained applying the Runge-Kutta 4 method. . def f_vec_ee(U,V): return numpy.multiply(dt, numpy.subtract(numpy.multiply(V, numpy.add(k0, numpy.divide(numpy.multiply(U,U), numpy.add(1., numpy.multiply(U,U))))), U)) . def f_vec_rk(U, V): f_vec = lambda U, V: numpy.subtract(numpy.multiply(V, numpy.add(k0, numpy.divide(numpy.multiply(U,U), numpy.add(1., numpy.multiply(U,U))))), U) k1 = f_vec(U, V) k2 = f_vec(U + numpy.multiply(dt/2., k1), V - numpy.multiply(dt/2., k1)) k3 = f_vec(U + numpy.multiply(dt/2., k2), V - numpy.multiply(dt/2., k2)) k4 = f_vec(U + numpy.multiply(dt, k3), V - numpy.multiply(dt, k3)) return numpy.multiply(dt/6., k1 + numpy.multiply(2., k2) + numpy.multiply(2., k3) + k4) . U_record_ee = numpy.empty(shape=(N,J)) V_record_ee = numpy.empty(shape=(N,J)) U_record_rk = numpy.empty(shape=(N,J)) V_record_rk = numpy.empty(shape=(N,J)) U_record_ee[0][:] = U[:] V_record_ee[0][:] = V[:] U_record_rk[0][:] = U[:] V_record_rk[0][:] = V[:] for ti in range(1,N): U_record_ee[ti][:] = numpy.linalg.solve(A_u, B_u.dot(U_record_ee[ti-1][:]) + f_vec_ee(U_record_ee[ti-1][:],V_record_ee[ti-1][:])) V_record_ee[ti][:] = numpy.linalg.solve(A_v, B_v.dot(V_record_ee[ti-1][:]) - f_vec_ee(U_record_ee[ti-1][:],V_record_ee[ti-1][:])) U_record_rk[ti][:] = numpy.linalg.solve(A_u, B_u.dot(U_record_rk[ti-1][:]) + f_vec_rk(U_record_rk[ti-1][:],V_record_rk[ti-1][:])) V_record_rk[ti][:] = numpy.linalg.solve(A_v, B_v.dot(V_record_rk[ti-1][:]) - f_vec_rk(U_record_rk[ti-1][:],V_record_rk[ti-1][:])) . The initial protein mass in our system: . print &#39;Explicit Euler&#39;, numpy.sum(numpy.multiply(dx, U_record_ee[0]) + numpy.multiply(dx, V_record_ee[0])) print &#39;Runge-Kutta 4 &#39;, numpy.sum(numpy.multiply(dx, U_record_ee[0]) + numpy.multiply(dx, V_record_ee[0])) . Since our reaction-diffusion system preserves mass, we should retain the same protein mass at steady-state for both numerical approaches: . print &#39;Explicit Euler %.14f&#39; % numpy.sum(numpy.multiply(dx, U_record_ee[-1]) + numpy.multiply(dx, V_record_ee[-1])) print &#39;Runge-Kutta 4 %.14f&#39; % numpy.sum(numpy.multiply(dx, U_record_rk[-1]) + numpy.multiply(dx, V_record_rk[-1])) . We realize that the difference between the two numerical methods is neglibible and we shall compare both approaches for a stiffer system another time. . A plot of the steady-state concentration profiles confirms that we cannot observe a significant differences between the results generated by both methods (varying J and N paints the same pictures). . pyplot.ylim((0., 2.1)) pyplot.xlabel(&#39;x&#39;) pyplot.ylabel(&#39;concentration&#39;) pyplot.plot(x_grid, U_record_ee[-1]) pyplot.plot(x_grid, V_record_ee[-1]) pyplot.plot(x_grid, U_record_rk[-1]) pyplot.plot(x_grid, V_record_rk[-1]) pyplot.show() . Kymograph of U integrated with the explicit Euler method. . fig, ax = pyplot.subplots() pyplot.xlabel(&#39;x&#39;) pyplot.ylabel(&#39;t&#39;) pyplot.ylim((0., T)) heatmap = ax.pcolormesh(x_grid, t_grid, U_record_ee, vmin=0., vmax=1.2) colorbar = pyplot.colorbar(heatmap) colorbar.set_label(&#39;concentration U&#39;) . Kymograph of U integrated with the Runge-Kutta 4 method. . fig, ax = pyplot.subplots() pyplot.xlabel(&#39;x&#39;) pyplot.ylabel(&#39;t&#39;) pyplot.ylim((0., T)) heatmap = ax.pcolormesh(x_grid, t_grid, U_record_rk, vmin=0., vmax=1.2) colorbar = pyplot.colorbar(heatmap) colorbar.set_label(&#39;concentration U&#39;) .",
            "url": "https://georg.io/2014/01/09/Crank_Nicolson_Runge_Kutta",
            "relUrl": "/2014/01/09/Crank_Nicolson_Runge_Kutta",
            "date": " • Jan 9, 2014"
        }
        
    
  
    
        ,"post7": {
            "title": "The Crank-Nicolson method applied to an accelerating domain in Python",
            "content": "Reaction Diffusion System on an Accelerating Domain . In a previous blog post we derived equations that describe the time behaviour of a reaction-diffusion system on a growing space domain. . In that work we assumed that the velocity of domain growth is an increasing function of one of the two unknown variables (here, protein concentrations) described by our reaction diffusion system. . Let us add another layer to this problem and assume that not the growth velocity is dependend on the unknown protein concentration but that the growth acceleration is a function of this unknown concentration. . We derived the equations that describe the time dynamics of our previous (velocity) problem in material coordinates here and reproduce them for clarity: . $$ frac{ partial u}{ partial t} = frac{D_u}{g^2} frac{ partial^2 u}{ partial X^2} - left( D_u frac{g_X}{g^3} + frac{a}{g} right) frac{ partial u}{ partial X} + f(u,v) - frac{a_X}{g} u,$$ . $$ frac{ partial v}{ partial t} = frac{D_v}{g^2} frac{ partial^2 v}{ partial X^2} - left( D_v frac{g_X}{g^3} + frac{a}{g} right) frac{ partial v}{ partial X} - f(u,v) - frac{a_X}{g} v,$$ . $$ frac{ partial g}{ partial t} = frac{ partial a}{ partial X},$$ . taken with initial and boundary conditions for $u$, $v$, and $g$ as described earlier. . Let us now introduce a fourth equation to this system to describe the time behaviour of the growth velocity $a$ as a function of its acceleration: . $$ frac{ partial a}{ partial t} = s(X,t),$$ . where $s(X,t)$ is the local growth acceleration. . In this expanded system of partial differential equations the time behaviour of the, now, unknown variable $a$ is described by an uncoupled partial differential equation. Since there is no spatial coupling in this partial differential equation, we do not need to impose any boundary conditions. As initial condition, we will assume that the system is at rest at first: . $$a(X,0) = 0.$$ . Let us now modify our previous code to simulate this expanded system. . Most of this code is just a copy-and-paste of our previous code so please refer to our comments there if anything is unclear. . %matplotlib inline . import numpy from scipy import integrate from matplotlib import pyplot numpy.set_printoptions(precision=3) . L = 1. J = 100 dX = float(L)/float(J) X_grid = numpy.array([float(dX)/2. + j*dX for j in range(J)]) x_grid = numpy.array([j*dX for j in range(J+1)]) T = 200 N = 1000 dt = float(T)/float(N-1) t_grid = numpy.array([n*dt for n in range(N)]) . D_v = float(10.)/float(100.) D_u = 0.01 * D_v . k0 = 0.067 f_vec = lambda U, V: numpy.multiply(dt, numpy.subtract(numpy.multiply(V, numpy.add(k0, numpy.divide(numpy.multiply(U,U), numpy.add(1., numpy.multiply(U,U))))), U)) . The initial condition for the growth velocity a is set to zero everywhere so that the space domain is at rest at first. . The expression for the growth acceleration s is the exact same expression we used earlier for the growth velocity. . total_protein = 2.26 no_high = 10 U = numpy.array([0.1 for i in range(no_high,J)] + [2. for i in range(0,no_high)]) V = numpy.array([float(total_protein-dX*sum(U))/float(J*dX) for i in range(0,J)]) g = numpy.array([1. for j in range(J)]) a = numpy.array([0. for j in range(J)]) s = lambda U: numpy.array([0.001*X_grid[j]*U[j] for j in range(J)]) . This is the polarized initial condition we choose for our concentration system U and V: . pyplot.ylim((0., 2.1)) pyplot.xlabel(&#39;X&#39;); pyplot.ylabel(&#39;concentration&#39;) pyplot.plot(X_grid, U) pyplot.plot(X_grid, V) pyplot.show() . As before, the initial condition for the slope g of the trajectories $G$ is 1. everywhere: . pyplot.plot(X_grid, g) pyplot.xlabel(&#39;X&#39;); pyplot.ylabel(&#39;g&#39;) pyplot.show() . Initially, s looks the same as a did in our previous code, while in this version of our code a is set initially to 0. everywhere: . pyplot.plot(X_grid, a) pyplot.plot(X_grid, s(U)) pyplot.xlabel(&#39;X&#39;); pyplot.ylabel(&#39;g&#39;) pyplot.ylim((-.0001, 0.0021)) pyplot.show() . # syntax to get one element in an array: http://stackoverflow.com/a/7332880/3078529 a_left = lambda a: numpy.concatenate((a[1:J], a[J-1:J])) a_right = lambda a: numpy.concatenate((a[0:1], a[0:J-1])) . f_vec_u = lambda U, V, g, a: numpy.subtract(f_vec(U, V), numpy.multiply(numpy.subtract(a_left(a), a_right(a)), numpy.multiply(float(dt)/(float(2.*dX)), numpy.divide(U,g)))) f_vec_v = lambda U, V, g, a: numpy.subtract(numpy.multiply(-1., f_vec(U, V)), numpy.multiply(numpy.subtract(a_left(a), a_right(a)), numpy.multiply(float(dt)/(float(2.*dX)), numpy.divide(V,g)))) . sigma_u_func = lambda g: numpy.divide(float(D_u*dt)/float(2.*dX*dX), numpy.multiply(g, g)) sigma_v_func = lambda g: numpy.divide(float(D_v*dt)/float(2.*dX*dX), numpy.multiply(g, g)) . # syntax to get one element in an array: http://stackoverflow.com/a/7332880/3078529 g_left = lambda g: numpy.concatenate((g[1:J], g[J-1:J])) g_right = lambda g: numpy.concatenate((g[0:1], g[0:J-1])) . rho_u_func = lambda g, a: numpy.multiply(float(-dt)/float(4.*dX), numpy.add(numpy.divide(a, g), numpy.multiply(numpy.subtract(g_left(g), g_right(g)), numpy.divide(float(D_u)/(2.*dX), numpy.power(g, 3))))) rho_v_func = lambda g, a: numpy.multiply(float(-dt)/float(4.*dX), numpy.add(numpy.divide(a, g), numpy.multiply(numpy.subtract(g_left(g), g_right(g)), numpy.divide(float(D_v)/(2.*dX), numpy.power(g, 3))))) . Note that here we use slightly more meaningful variable names for the ODE steppers for g and x_grid than in our previous code. . The ODE stepper for a is of the same form except that the right-hand side is governed by the growth acceleration s: . g_rhs = lambda t, g, a: numpy.divide(numpy.subtract(a_left(a), a_right(a)), numpy.array([dX]+[2.*dX for j in range(J-2)]+[dX])) g_stepper = integrate.ode(g_rhs) g_stepper = g_stepper.set_integrator(&#39;dopri5&#39;, nsteps=10, max_step=dt) g_stepper = g_stepper.set_initial_value(g, 0.) g_stepper = g_stepper.set_f_params(a) . a_rhs = lambda t, a, s: s a_stepper = integrate.ode(a_rhs) a_stepper = a_stepper.set_integrator(&#39;dopri5&#39;, nsteps=10, max_step=dt) a_stepper = a_stepper.set_initial_value(a, 0.) a_stepper = a_stepper.set_f_params(s(U)) . x_grid_rhs = lambda t, x_grid, a: numpy.concatenate(([0.], numpy.divide(numpy.add(a[0:J-1], a[1:J]), 2.), a[J-1:J])) x_stepper = integrate.ode(x_grid_rhs) x_stepper = x_stepper.set_integrator(&#39;dopri5&#39;, nsteps=10, max_step=dt) x_stepper = x_stepper.set_initial_value(x_grid, 0.) x_stepper = x_stepper.set_f_params(a) . U_record = [] V_record = [] g_record = [] a_record = [] x_record = [] U_record.append(U) V_record.append(V) g_record.append(g) a_record.append(a) x_record.append(x_grid) . Let us now integrate this system for a total of N time points. . Notice that we update the ODE stepper of g, a and x_grid every time step with the current state of their corresponding right-hand-side expressions. This is done with a call to .set_f_params() on the corresponding objects. . for ti in range(1,N): sigma_u = sigma_u_func(g) sigma_v = sigma_v_func(g) rho_u = rho_u_func(g, a) rho_v = rho_v_func(g, a) A_u = numpy.diagflat([-sigma_u[j]+rho_u[j] for j in range(1,J)], -1) + numpy.diagflat([1.+sigma_u[0]+rho_u[0]]+[1.+2.*sigma_u[j] for j in range(1,J-1)]+[1.+sigma_u[J-1]-rho_u[J-1]]) + numpy.diagflat([-(sigma_u[j]+rho_u[j]) for j in range(0,J-1)], 1) B_u = numpy.diagflat([sigma_u[j]-rho_u[j] for j in range(1,J)], -1) + numpy.diagflat([1.-sigma_u[0]-rho_u[0]]+[1.-2.*sigma_u[j] for j in range(1,J-1)]+[1.-sigma_u[J-1]+rho_u[J-1]]) + numpy.diagflat([sigma_u[j]+rho_u[j] for j in range(0,J-1)], 1) A_v = numpy.diagflat([-sigma_v[j]+rho_v[j] for j in range(1,J)], -1) + numpy.diagflat([1.+sigma_v[0]+rho_v[0]]+[1.+2.*sigma_v[j] for j in range(1,J-1)]+[1.+sigma_v[J-1]-rho_v[J-1]]) + numpy.diagflat([-(sigma_v[j]+rho_v[j]) for j in range(0,J-1)], 1) B_v = numpy.diagflat([sigma_v[j]-rho_v[j] for j in range(1,J)], -1) + numpy.diagflat([1.-sigma_v[0]-rho_v[0]]+[1.-2.*sigma_v[j] for j in range(1,J-1)]+[1.-sigma_v[J-1]+rho_v[J-1]]) + numpy.diagflat([sigma_v[j]+rho_v[j] for j in range(0,J-1)], 1) U_new = numpy.linalg.solve(A_u, B_u.dot(U) + f_vec_u(U, V, g, a)) V_new = numpy.linalg.solve(A_v, B_v.dot(V) + f_vec_v(U, V, g, a)) while a_stepper.successful() and a_stepper.t + dt &lt; ti*dt: a_stepper.integrate(a_stepper.t + dt) while g_stepper.successful() and g_stepper.t + dt &lt; ti*dt: g_stepper.integrate(g_stepper.t + dt) while x_stepper.successful() and x_stepper.t + dt &lt; ti*dt: x_stepper.integrate(x_stepper.t + dt) g_stepper = g_stepper.set_f_params(a) x_stepper = x_stepper.set_f_params(a) a_stepper = a_stepper.set_f_params(s(U)) U = U_new V = V_new # these are the correct &quot;y&quot; values to save for the current time step since # we integrate only up to t &lt; ti*dt g = g_stepper.y a = a_stepper.y x_grid = x_stepper.y U_record.append(U) V_record.append(V) g_record.append(g) a_record.append(a) x_record.append(x_grid) . As in our previous code protein mass is conserved by definition of our boundary conditions for U and V. . To make certain that our numerical integration does conserve protein mass (at least to some sensible degree) we work out the initial total mass and final total mass respectively: . sum(numpy.multiply(numpy.diff(x_record[0]),U_record[0]) + numpy.multiply(numpy.diff(x_record[0]),V_record[0])) . sum(numpy.multiply(numpy.diff(x_record[-1]),U_record[-1]) + numpy.multiply(numpy.diff(x_record[-1]),V_record[-1])) . These numbers look good and we therefore assume that our numerical integration is somewhat stable. . A look at a kymograph of the concentration of U reveals that the initial condition on U is lost quickly - which will be due to reasons we discussed earlier. . U_record = numpy.array(U_record) V_record = numpy.array(V_record) fig, ax = pyplot.subplots() pyplot.xlabel(&#39;X&#39;); pyplot.ylabel(&#39;t&#39;) heatmap = ax.pcolormesh(x_record[0], t_grid, U_record, vmin=0., vmax=1.2) colorbar = pyplot.colorbar(heatmap) colorbar.set_label(&#39;concentration U&#39;) . Since with the above setup we only introduce acceleration and no deceleration, we are not surprised to observe that the growth velocity simply increases over time: . a_record = numpy.array(a_record) fig, ax = pyplot.subplots() pyplot.xlabel(&#39;X&#39;); pyplot.ylabel(&#39;t&#39;) heatmap = ax.pcolormesh(X_grid, t_grid, a_record) colorbar = pyplot.colorbar(heatmap) colorbar.set_label(&#39;local growth velocity a(X,t)&#39;) . This continual increase in growth velocity is only due to an initial acceleration due to the initial condition we imposed on U: . fig, ax = pyplot.subplots() pyplot.xlabel(&#39;X&#39;); pyplot.ylabel(&#39;t&#39;) s_record = numpy.empty(shape=(N, J)) for ti in range(N): s_record[ti] = s(U_record[ti]) heatmap = ax.pcolormesh(X_grid, t_grid, s_record) colorbar = pyplot.colorbar(heatmap) colorbar.set_label(&#39;local growth acceleration s&#39;) .",
            "url": "https://georg.io/2014/01/08/Reaction_Diffusion_Accelerating_Domain",
            "relUrl": "/2014/01/08/Reaction_Diffusion_Accelerating_Domain",
            "date": " • Jan 8, 2014"
        }
        
    
  
    
        ,"post8": {
            "title": "Document clustering applied to scientific articles in Python",
            "content": "The Ten Most Similar PLoS Biology Articles . ... at least by some measure. . I recently downloaded 1754 PLoS Biology articles as XML files through the PLoS API and have looked at the distribution of the time to publication of PLoS Biology and other PLoS journals. . Here I will play a little with scikit-learn to see if I can discover those PLoS Biology articles (in my data set) that are most similar to one another. . Import Packages . I started writing a Python package (PLoSPy) for more convient parsing of the XML files I have download from PLoS. . import plospy import os from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.metrics.pairwise import linear_kernel import itertools . Discover Data Files on Hard Disk . all_names = [name for name in os.listdir(&#39;../plos/plos_biology/plos_biology_data&#39;) if &#39;.dat&#39; in name] . all_names[0:10] . print len(all_names) . Vectorize all Articles . To reduce memory use, I wrote the following method that returns an iterator over all article bodies. In passing this iterator to the vectorizer, we avoid loading all articles into memory at once - despite the use of an iterator here, I have not been able to repeat this experiment with all 65,000-odd PLoS ONE articles without running out of memory. . ids = [] titles = [] def get_corpus(all_names): for name_i, name in enumerate(all_names): docs = plospy.PlosXml(&#39;../plos/plos_biology/plos_biology_data/&#39;+name) for article in docs.docs: ids.append(article[&#39;id&#39;]) titles.append(article[&#39;title&#39;]) yield article[&#39;body&#39;] . corpus = get_corpus(all_names) tfidf = TfidfVectorizer().fit_transform(corpus) . Just as a sanity check, the number of DOIs in our data set should now equal 1754 as this is the number of articles I downloaded in the first place. . len(ids) . The vectorizer generated a matrix with 139,748 columns (these are the tokens, i.e. probably unique words used in all 1754 PLoS Biology articles) and 1754 rows (corresponding to individual articles). . tfidf.shape . Let us now compute all pairwise cosine distances betweeen all 1754 vectors (articles) in matrix tfidf. I copied and pasted most of this from a StackOverflow answer that I cannot find now - I will add a link to the answer when I come across it again. . To get the ten most similar articles, we track the top five pairwise matches. . top_five = [[-1,-1,-1] for i in range(5)] threshold = -1. for index in range(len(ids)): cosine_similarities = linear_kernel(tfidf[index:index+1], tfidf).flatten() related_docs_indices = cosine_similarities.argsort()[:-5:-1] first = related_docs_indices[0] second = related_docs_indices[1] if first != index: print &#39;Error&#39; break if cosine_similarities[second] &gt; threshold: if first not in [top[0] for top in top_five] and first not in [top[1] for top in top_five]: scores = [top[2] for top in top_five] replace = scores.index(min(scores)) # print &#39;replace&#39;,replace top_five[replace] = [first, second, cosine_similarities[second]] # print &#39;old threshold&#39;,threshold threshold = min(scores) # print &#39;new threshold&#39;,threshold . The Most Similar Articles . Let us now take a look at the results! . for tf in top_five: print &#39;&#39; print(&#39;Cosine Similarity: %.2f&#39; % tf[2]) print(&#39;Title 1: %s&#39; %titles[tf[0]]) print(&#39;http://www.plosbiology.org/article/info%3Adoi%2F&#39;+str(ids[tf[0]])) print &#39;&#39; print(&#39;Title 2: %s&#39; %titles[tf[1]]) print(&#39;http://www.plosbiology.org/article/info%3Adoi%2F&#39;+str(ids[tf[1]])) print &#39;&#39; .",
            "url": "https://georg.io/2013/12/16/Most_Similar_PLoS_Biology",
            "relUrl": "/2013/12/16/Most_Similar_PLoS_Biology",
            "date": " • Dec 16, 2013"
        }
        
    
  
    
        ,"post9": {
            "title": "The Crank-Nicolson method implemented from scratch in Python",
            "content": "The Crank-Nicolson Method . The Crank-Nicolson method is a well-known finite difference method for the numerical integration of the heat equation and closely related partial differential equations. . We often resort to a Crank-Nicolson (CN) scheme when we integrate numerically reaction-diffusion systems in one space dimension . $$ frac{ partial u}{ partial t} = D frac{ partial^2 u}{ partial x^2} + f(u),$$ . $$ frac{ partial u}{ partial x} Bigg|_{x = 0, L} = 0,$$ . where $u$ is our concentration variable, $x$ is the space variable, $D$ is the diffusion coefficient of $u$, $f$ is the reaction term, and $L$ is the length of our one-dimensional space domain. . Note that we use Neumann boundary conditions and specify that the solution $u$ has zero space slope at the boundaries, effectively prohibiting entrance or exit of material at the boundaries (no-flux boundary conditions). . Finite Difference Methods . Many fantastic textbooks and tutorials have been written about finite difference methods, for instance a free textbook by Lloyd Trefethen. . Here we describe a few basic aspects of finite difference methods. . The above reaction-diffusion equation describes the time evolution of variable $u(x,t)$ in one space dimension ($u$ is a line concentration). If we knew an analytic expression for $u(x,t)$ then we could plot $u$ in a two-dimensional coordinate system with axes $t$ and $x$. . To approximate $u(x,t)$ numerically we discretize this two-dimensional coordinate system resulting, in the simplest case, in a two-dimensional regular grid. This picture is employed commonly when constructing finite differences methods, see for instance Figure 3.2.1 of Trefethen. . Let us discretize both time and space as follows: . $$t_n = n Delta t,~ n = 0, ldots, N-1,$$ . $$x_j = j Delta x,~ j = 0, ldots, J-1,$$ . where $N$ and $J$ are the number of discrete time and space points in our grid respectively. $ Delta t$ and $ Delta x$ are the time step and space step respectively and defined as follows: . $$ Delta t = T / N,$$ . $$ Delta x = L / J,$$ . where $T$ is the point in time up to which we will integrate $u$ numerically. . Our ultimate goal is to construct a numerical method that allows us to approximate the unknonwn analytic solution $u(x,t)$ reasonably well in these discrete grid points. . That is we want construct a method that computes values $U(j Delta x, n Delta t)$ (note: capital $U$) so that . $$U(j Delta x, n Delta t) approx u(j Delta x, n Delta t)$$ . As a shorthand we will write $U_j^n = U(j Delta x, n Delta t)$ and $(j,n)$ to refer to grid point $(j Delta x, n Delta t)$. . The Crank-Nicolson Stencil . Based on the two-dimensional grid we construct we then approximate the operators of our reaction-diffusion system. . For instance, to approximate the time derivative on the left-hand side in grid point $(j,n)$ we use the values of $U$ in two specific grid points: . $$ frac{ partial u}{ partial t} Bigg|_{x = j Delta x, t = n Delta t} approx frac{U_j^{n+1} - U_j^n}{ Delta t}.$$ . We can think of this scheme as a stencil that we superimpose on our $(x,t)$-grid and this particular stencil is commonly referred to as forward difference. . The spatial part of the Crank-Nicolson stencil (or see Table 3.2.2 of Trefethen) for the heat equation ($u_t = u_{xx}$) approximates the Laplace operator of our equation and takes the following form . $$ frac{ partial^2 u}{ partial x^2} Bigg|_{x = j Delta x, t = n Delta t} approx frac{1}{2 Delta x^2} left( U_{j+1}^n - 2 U_j^n + U_{j-1}^n + U_{j+1}^{n+1} - 2 U_j^{n+1} + U_{j-1}^{n+1} right).$$ . To approximate $f(u(j Delta x, n Delta t))$ we write simply $f(U_j^n)$. . These approximations define the stencil for our numerical method as pictured on Wikipedia. . . Applying this stencil to grid point $(j,n)$ gives us the following approximation of our reaction-diffusion equation: . $$ frac{U_j^{n+1} - U_j^n}{ Delta t} = frac{D}{2 Delta x^2} left( U_{j+1}^n - 2 U_j^n + U_{j-1}^n + U_{j+1}^{n+1} - 2 U_j^{n+1} + U_{j-1}^{n+1} right) + f(U_j^n).$$ . Reordering Stencil into Linear System . Let us define $ sigma = frac{D Delta t}{2 Delta x^2}$ and reorder the above approximation of our reaction-diffusion equation: . $$- sigma U_{j-1}^{n+1} + (1+2 sigma) U_j^{n+1} - sigma U_{j+1}^{n+1} = sigma U_{j-1}^n + (1-2 sigma) U_j^n + sigma U_{j+1}^n + Delta t f(U_j^n).$$ . This equation makes sense for space indices $j = 1, ldots,J-2$ but it does not make sense for indices $j=0$ and $j=J-1$ (on the boundaries): . $$j=0:~- sigma U_{-1}^{n+1} + (1+2 sigma) U_0^{n+1} - sigma U_{1}^{n+1} = sigma U_{-1}^n + (1-2 sigma) U_0^n + sigma U_{1}^n + Delta t f(U_0^n),$$ . $$j=J-1:~- sigma U_{J-2}^{n+1} + (1+2 sigma) U_{J-1}^{n+1} - sigma U_{J}^{n+1} = sigma U_{J-2}^n + (1-2 sigma) U_{J-1}^n + sigma U_{J}^n + Delta t f(U_{J-1}^n).$$ . The problem here is that the values $U_{-1}^n$ and $U_J^n$ lie outside our grid. . However, we can work out what these values should equal by considering our Neumann boundary condition. Let us discretize our boundary condition at $j=0$ with the backward difference and at $j=J-1$ with the forward difference: . $$ frac{U_1^n - U_0^n}{ Delta x} = 0,$$ . $$ frac{U_J^n - U_{J-1}^n}{ Delta x} = 0.$$ . These two equations make it clear that we need to amend our above numerical approximation for $j=0$ with the identities $U_0^n = U_1^n$ and $U_0^{n+1} = U_1^{n+1}$, and for $j=J-1$ with the identities $U_{J-1}^n = U_J^n$ and $U_{J-1}^{n+1} = U_J^{n+1}$. . Let us reinterpret our numerical approximation of the line concentration of $u$ in a fixed point in time as a vector $ mathbf{U}^n$: . $$ mathbf{U}^n = begin{bmatrix} U_0^n vdots U_{J-1}^n end{bmatrix}.$$Using this notation we can now write our above approximation for a fixed point in time, $t = n Delta t$, compactly as a linear system: . $$ begin{bmatrix} 1+ sigma &amp; - sigma &amp; 0 &amp; 0 &amp; 0 &amp; cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 - sigma &amp; 1+2 sigma &amp; - sigma &amp; 0 &amp; 0 &amp; cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 0 &amp; - sigma &amp; 1+2 sigma &amp; - sigma &amp; cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; ddots &amp; ddots &amp; ddots &amp; ddots &amp; 0 &amp; 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; - sigma &amp; 1+2 sigma &amp; - sigma 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; - sigma &amp; 1+ sigma end{bmatrix} begin{bmatrix} U_0^{n+1} U_1^{n+1} U_2^{n+1} vdots U_{J-2}^{n+1} U_{J-1}^{n+1} end{bmatrix} = begin{bmatrix} 1- sigma &amp; sigma &amp; 0 &amp; 0 &amp; 0 &amp; cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 sigma &amp; 1-2 sigma &amp; sigma &amp; 0 &amp; 0 &amp; cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 0 &amp; sigma &amp; 1-2 sigma &amp; sigma &amp; cdots &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; ddots &amp; ddots &amp; ddots &amp; ddots &amp; 0 &amp; 0 &amp; 0 &amp; 0 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; sigma &amp; 1-2 sigma &amp; sigma 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; sigma &amp; 1- sigma end{bmatrix} begin{bmatrix} U_0^{n} U_1^{n} U_2^{n} vdots U_{J-2}^{n} U_{J-1}^{n} end{bmatrix} + begin{bmatrix} Delta t f(U_0^n) Delta t f(U_1^n) Delta t f(U_2^n) vdots Delta t f(U_{J-2}^n) Delta t f(U_{J-1}^n) end{bmatrix}. $$Note that since our numerical integration starts with a well-defined initial condition at $n=0$, $ mathbf{U}^0$, the vector $ mathbf{U}^{n+1}$ on the left-hand side is the only unknown in this system of linear equations. . Thus, to integrate numerically our reaction-diffusion system from time point $n$ to $n+1$ we need to solve numerically for vector $ mathbf{U}^{n+1}$. . Let us call the matrix on the left-hand side $A$, the one on the right-hand side $B$, and the vector on the right-hand side $ mathbf{f}^n$. Using this notation we can write the above system as . $$A mathbf{U}^{n+1} = B mathbf{U}^n + f^n.$$ . In this linear equation, matrices $A$ and $B$ are defined by our problem: we need to specify these matrices once for our problem and incorporate our boundary conditions in them. Vector $ mathbf{f}^n$ is a function of $ mathbf{U}^n$ and so needs to be reevaluated in every time point $n$. We also need to carry out one matrix-vector multiplication every time point, $B mathbf{U}^n$, and one vector-vector addition, $B mathbf{U}^n + f^n$. . The most expensive numerical operation is inversion of matrix $A$ to solve for $ mathbf{U}^{n+1}$, however we may get away with doing this only once and store the inverse of $A$ as $A^{-1}$: . $$ mathbf{U}^{n+1} = A^{-1} left( B mathbf{U}^n + f^n right).$$ . A Crank-Nicolson Example in Python . Let us apply the CN method to a two-variable reaction-diffusion system that was introduced by Mori et al.: . $$ frac{ partial u}{ partial t} = D_u frac{ partial^2 u}{ partial x^2} + f(u,v),$$ . $$ frac{ partial v}{ partial t} = D_v frac{ partial^2 v}{ partial x^2} - f(u,v),$$ . with Neumann boundary conditions . $$ frac{ partial u}{ partial x} Bigg|_{x=0,L} = 0,$$ . $$ frac{ partial v}{ partial x} Bigg|_{x=0,L} = 0.$$ . The variables of this system, $u$ and $v$, represent the concetrations of the active form and its inactive form respectively. The reaction term $f(u,v)$ describes the interchange (activation and inactivation) between these two states of the protein. A particular property of this system is that the inactive has much greater diffusivity that the active form, $D_v gg D_u$. . Using the CN method to integrate this system numerically, we need to set up two separate approximations . $$A_u mathbf{U}^{n+1} = B_u mathbf{U}^n + mathbf{f}^n,$$ . $$A_v mathbf{V}^{n+1} = B_v mathbf{V}^n - mathbf{f}^n,$$ . with two different $ sigma$ terms, $ sigma_u = frac{D_u Delta t}{2 Delta x^2}$ and $ sigma_v = frac{D_v Delta t}{2 Delta x^2}$. . Import Packages . For the matrix-vector multiplication, vector-vector addition, and matrix inversion that we will need to carry out we will use the Python library NumPy. To visualize our numerical solutions, we will use pyplot. . import numpy from matplotlib import pyplot . Numpy allows us to truncate the numerical values of matrices and vectors to improve their display with set_printoptions. . numpy.set_printoptions(precision=3) . Specify Grid . Our one-dimensional domain has unit length and we define J = 100 equally spaced grid points in this domain. This divides our domain into J-1 subintervals, each of length dx. . L = 1. J = 100 dx = float(L)/float(J-1) x_grid = numpy.array([j*dx for j in range(J)]) . Equally, we define N = 1000 equally spaced grid points on our time domain of length T = 200 thus dividing our time domain into N-1 intervals of length dt. . T = 200 N = 1000 dt = float(T)/float(N-1) t_grid = numpy.array([n*dt for n in range(N)]) . Specify System Parameters and the Reaction Term . We choose our parameter values based on the work by Mori et al.. . D_v = float(10.)/float(100.) D_u = 0.01 * D_v k0 = 0.067 f = lambda u, v: dt*(v*(k0 + float(u*u)/float(1. + u*u)) - u) g = lambda u, v: -f(u,v) sigma_u = float(D_u*dt)/float((2.*dx*dx)) sigma_v = float(D_v*dt)/float((2.*dx*dx)) total_protein = 2.26 . Specify the Initial Condition . As discussed by Mori et al., we can expect to observe interesting behaviour in the steady state of this system if we choose a heterogeneous initial condition for $u$. . Here, we initialize $u$ with a step-like heterogeneity: . no_high = 10 U = numpy.array([0.1 for i in range(no_high,J)] + [2. for i in range(0,no_high)]) V = numpy.array([float(total_protein-dx*sum(u))/float(J*dx) for i in range(0,J)]) . Note that we make certain that total protein amounts equal a certain value, total_protein. The importance of this was discussed by Walther et al.. . Let us plot our initial condition for confirmation: . ylim((0., 2.1)) xlabel(&#39;x&#39;); ylabel(&#39;concentration&#39;) pyplot.plot(x_grid, U) pyplot.plot(x_grid, V) pyplot.show() . The blue curve is the initial condition for $U$, stored in Python variable U, and the green curve is the initial condition for $V$ stored in V. . Create Matrices . The matrices that we need to construct are all tridiagonal so they are easy to construct with numpy.diagflat. . A_u = numpy.diagflat([-sigma_u for i in range(J-1)], -1) + numpy.diagflat([1.+sigma_u]+[1.+2.*sigma_u for i in range(J-2)]+[1.+sigma_u]) + numpy.diagflat([-sigma_u for i in range(J-1)], 1) B_u = numpy.diagflat([sigma_u for i in range(J-1)], -1) + numpy.diagflat([1.-sigma_u]+[1.-2.*sigma_u for i in range(J-2)]+[1.-sigma_u]) + numpy.diagflat([sigma_u for i in range(J-1)], 1) A_v = numpy.diagflat([-sigma_v for i in range(J-1)], -1) + numpy.diagflat([1.+sigma_v]+[1.+2.*sigma_v for i in range(J-2)]+[1.+sigma_v]) + numpy.diagflat([-sigma_v for i in range(J-1)], 1) B_v = numpy.diagflat([sigma_v for i in range(J-1)], -1) + numpy.diagflat([1.-sigma_v]+[1.-2.*sigma_v for i in range(J-2)]+[1.-sigma_v]) + numpy.diagflat([sigma_v for i in range(J-1)], 1) . To confirm, this is what A_u looks like: . print A_u . [[ 1.981 -0.981 0. ..., 0. 0. 0. ] [-0.981 2.962 -0.981 ..., 0. 0. 0. ] [ 0. -0.981 2.962 ..., 0. 0. 0. ] ..., [ 0. 0. 0. ..., 2.962 -0.981 0. ] [ 0. 0. 0. ..., -0.981 2.962 -0.981] [ 0. 0. 0. ..., 0. -0.981 1.981]] . Solve the System Iteratively . To advance our system by one time step, we need to do one matrix-vector multiplication followed by one vector-vector addition on the right hand side. . To facilitate this, we rewrite our reaction term so that it accepts concentration vectors $ mathbf{U}^n$ and $ mathbf{V}^n$ as arguments and returns vector $ mathbf{f}^n$. . As a reminder, this is our non-vectorial definition of $f$ . f = lambda u, v: v*(k0 + float(u*u)/float(1. + u*u)) - u . f_vec = lambda U, V: numpy.multiply(dt, numpy.subtract(numpy.multiply(V, numpy.add(k0, numpy.divide(numpy.multiply(U,U), numpy.add(1., numpy.multiply(U,U))))), U)) . Let us make certain that this produces the same values as our non-vectorial f: . print f(U[0], V[0]) . 0.00996135898275 . print f(U[-1], V[-1]) . -0.0623832232232 . print f_vec(U, V) . [ 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 -0.062 -0.062 -0.062 -0.062 -0.062 -0.062 -0.062 -0.062 -0.062 -0.062] . Accounting for rounding of the displayed values due to the set_printoptions we set above, we can see that f and f_vec generate the same values for our initial condition at both ends of our domain. . We will use numpy.linalg.solve to solve our linear system each time step. . While we integrate our system over time we will record both U and V at each time step in U_record and V_record respectively so that we can plot our numerical solutions over time. . U_record = [] V_record = [] U_record.append(U) V_record.append(V) for ti in range(1,N): U_new = numpy.linalg.solve(A_u, B_u.dot(U) + f_vec(U,V)) V_new = numpy.linalg.solve(A_v, B_v.dot(V) - f_vec(U,V)) U = U_new V = V_new U_record.append(U) V_record.append(V) . Plot the Numerical Solution . Let us take a look at the numerical solution we attain after N time steps. . ylim((0., 2.1)) xlabel(&#39;x&#39;); ylabel(&#39;concentration&#39;) pyplot.plot(x_grid, U) pyplot.plot(x_grid, V) pyplot.show() . And here is a kymograph of the values of U. This plot shows concisely the behaviour of U over time and we can clear observe the wave-pinning behaviour described by Mori et al.. Furthermore, we observe that this wave pattern is stable for about 50 units of time and we therefore conclude that this wave pattern is a stable steady state of our system. . U_record = numpy.array(U_record) V_record = numpy.array(V_record) fig, ax = subplots() xlabel(&#39;x&#39;); ylabel(&#39;t&#39;) heatmap = ax.pcolor(x_grid, t_grid, U_record, vmin=0., vmax=1.2) .",
            "url": "https://georg.io/2013/12/03/Crank_Nicolson",
            "relUrl": "/2013/12/03/Crank_Nicolson",
            "date": " • Dec 3, 2013"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Hi, I’m Georg Walther and here I blog about all things tech and data that I find interesting. . For lack of creativity, here is a rundown of the tools I use and projects I do. Maybe someday I’ll come up with a more interesting bio. . See more about me on my LinkedIn page or drop me a line on Twitter. . In case you prefer speaking directly about the tech or data of your business, feel free to grab an appointment with me over at Datensprechstunde (German) or Data Consultation (English). . Industry experience . Data science / machine learning experience . Rare event (e.g. click, conversion, purchase, email open and reaction, generic user action) prediction from user-level time series data using classical machine learning and deep learning | User clustering based on web tracking time series data | Data-driven SEM, display, real-time bidding, and direct marketing optimization | Time-series anomaly detection and reporting for streamed and batched sensor data | Agent-based asset trading (reinforcement learning) | Basket-based recommender systems | Predictive analytics / forecasting of asset demand based on marketplace user behaviour (big data machine learning) | Use of high-dimensional acceleration time series data to solve a classification problem | Causal inference using propensity scores and clustering to estimate treatment effects in high-dimensional acceleration time series data | . DevOps experience . Implementation of two-tiered deployment CI/CD process for a Python/Django/wagtail app | Set up scalable Azure-based infrastructure: Terraform templates for instantiation of managed Kubernetes cluster, database backend and virtual network security; Implementation of Kubernetes deployment and service objects as well as ingress resource and controller | . Full-stack experience . Ideation and implementation of a modern ReactJS web app for the financial sector | . Technologies . Machine Learning &amp; Data Science . Tensorflow, Keras, PyTorch, Numpy, OpenAI Gym, Pandas, Scikit-Learn, Scipy, Spark MLlib . Big Data . Presto, Apache Spark, PySpark, Spark SQL, Spark MLlib . DevOps . Apache Airflow, Ansible, Terraform, Azure Cloud, Google Cloud Platform, Kubernetes, NGINX kubernetes ingress controller, Docker, Docker-Compose, Heroku IaaS, GitLab CI . Frontend . JavaScript, ECMAScript 6, ReactJS . Backend . Apache2, Falcon, Gunicorn, Nginx, PyPy, Pytest, Python, Requests, Django, Wagtail, Flask, Keycloak . Cloud Computing . AWS, Google Cloud Platform, Azure, Azure Managed Kubernetes Service (AKS), AWS Elastic Container Service (ECS), AWS Elastic Kubernetes Service (EKS) . Frameworks / concepts / methods . Anomaly Detection, Continuous Delivery, Continuous Integration, DevOps, Model Selection (AIC, BIC), Reinforcement Learning, Supervised Learning, Unsupervised Learning, Frequency domain feature engineering (fast Fourier transformation / FFT, discrete Fourier transformation / DFT, short-term Fourier transformation / STFT), Causal Inference .",
          "url": "https://georg.io/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://georg.io/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}